{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source\n",
    "\n",
    "https://www.kaggle.com/datasets/luisblanche/covidct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:04:29.548474: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 16:04:29.583270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 16:04:30.291798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "# import tensorflow.keras.backend as K\n",
    "#tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def limitgpu(maxmem):\n",
    "\tgpus = tf.config.list_physical_devices('GPU')\n",
    "\tif gpus:\n",
    "\t\t# Restrict TensorFlow to only allocate a fraction of GPU memory\n",
    "\t\ttry:\n",
    "\t\t\tfor gpu in gpus:\n",
    "\t\t\t\ttf.config.experimental.set_virtual_device_configuration(gpu,\n",
    "\t\t\t\t\t\t[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=maxmem)])\n",
    "\t\texcept RuntimeError as e:\n",
    "\t\t\t# Virtual devices must be set before GPUs have been initialized\n",
    "\t\t\tprint(e)\n",
    "\n",
    "\n",
    "# 1.5GB\n",
    "limitgpu(1024+512) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.18.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:04:37.918455: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-10-30 16:04:37.918552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8783 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:19:00.0, compute capability: 7.5\n",
      "2024-10-30 16:04:37.919214: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 1\n",
      "2024-10-30 16:04:37.919263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14457 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2024-10-30 16:04:37.919797: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 2\n",
      "2024-10-30 16:04:37.919836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14457 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2024-10-30 16:04:37.920349: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 3\n",
      "2024-10-30 16:04:37.920386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13813 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime==0.1.1.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming images data to Specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Processing images for machine learning to create two-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing CT_scan Covid-19  CT_scan nonCovid-19 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_covid_features_df.iloc[0].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_noncovid_features_df.iloc[1].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the two data frame to create a dataset ready for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>89991</th>\n",
       "      <th>89992</th>\n",
       "      <th>89993</th>\n",
       "      <th>89994</th>\n",
       "      <th>89995</th>\n",
       "      <th>89996</th>\n",
       "      <th>89997</th>\n",
       "      <th>89998</th>\n",
       "      <th>89999</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Covid (14).png</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>65</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Covid (227).png</th>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>224</td>\n",
       "      <td>231</td>\n",
       "      <td>239</td>\n",
       "      <td>233</td>\n",
       "      <td>185</td>\n",
       "      <td>144</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-COVID-19_757.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-COVID-19_714.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-COVID-19_504.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COVID-19_0432.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Covid (254).png</th>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>197</td>\n",
       "      <td>194</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-COVID-19_046.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Covid (557).png</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>211</td>\n",
       "      <td>216</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>232</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Covid (711).png</th>\n",
       "      <td>205</td>\n",
       "      <td>204</td>\n",
       "      <td>206</td>\n",
       "      <td>208</td>\n",
       "      <td>204</td>\n",
       "      <td>202</td>\n",
       "      <td>198</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>218</td>\n",
       "      <td>229</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4649 rows Ã— 90001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0    1    2    3    4    5    6    7    8    9  ...  \\\n",
       "Non-Covid (14).png     64   64   64   64   63   63   63   63   63   63  ...   \n",
       "Non-Covid (227).png    63   63   64   64   65   65   65   65   65   66  ...   \n",
       "Non-COVID-19_757.png    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "Non-COVID-19_714.png    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "Non-COVID-19_504.png    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "...                   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "COVID-19_0432.png       0    0    0    0    0    0    0    0    0    0  ...   \n",
       "Covid (254).png       193  193  197  194  198  198  196  196  197  195  ...   \n",
       "Non-COVID-19_046.png    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "Non-Covid (557).png    61   61   61   64   63   63   63   63   63   63  ...   \n",
       "Covid (711).png       205  204  206  208  204  202  198  205  205  203  ...   \n",
       "\n",
       "                      89991  89992  89993  89994  89995  89996  89997  89998  \\\n",
       "Non-Covid (14).png       63     63     63     62     63     63     65     67   \n",
       "Non-Covid (227).png     210    211    224    231    239    233    185    144   \n",
       "Non-COVID-19_757.png      0      0      0      0      0      0      0      0   \n",
       "Non-COVID-19_714.png      0      0      0      0      0      0      0      0   \n",
       "Non-COVID-19_504.png      0      0      0      0      0      0      0      0   \n",
       "...                     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "COVID-19_0432.png         0      0      0      0      0      0      0      0   \n",
       "Covid (254).png         255    255    255    255    255    255    255    255   \n",
       "Non-COVID-19_046.png      0      0      0      0      0      0      0      0   \n",
       "Non-Covid (557).png     210    209    208    211    216    222    229    232   \n",
       "Covid (711).png         209    209    215    215    218    229    232    231   \n",
       "\n",
       "                      89999  class_label  \n",
       "Non-Covid (14).png       68            0  \n",
       "Non-Covid (227).png     112            0  \n",
       "Non-COVID-19_757.png      0            0  \n",
       "Non-COVID-19_714.png      0            0  \n",
       "Non-COVID-19_504.png      0            0  \n",
       "...                     ...          ...  \n",
       "COVID-19_0432.png         0            1  \n",
       "Covid (254).png         255            1  \n",
       "Non-COVID-19_046.png      0            0  \n",
       "Non-Covid (557).png     232            0  \n",
       "Covid (711).png         229            1  \n",
       "\n",
       "[4649 rows x 90001 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_x Data Shape: \n",
      "(2173, 300, 300, 1)\n",
      "Output_y Data Shape: \n",
      "(2173, 1)\n"
     ]
    }
   ],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_x Data Shape: \n",
      "(2476, 300, 300, 1)\n",
      "Output_y Data Shape: \n",
      "(2476, 1)\n"
     ]
    }
   ],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x__.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y__.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_x Data Shape: \n",
      "(4649, 300, 300, 1)\n",
      "Output_y Data Shape: \n",
      "(4649, 2)\n"
     ]
    }
   ],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Negative Covid-19\n",
    "for i in range(10):\n",
    "    imshow(input_data_x_[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Positive Covid-19\n",
    "for i in range(15):\n",
    "    imshow(input_data_x__[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize first 5 images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    imshow(input_data_x[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "#### **Splitting the dataset into the Training set and Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_, test_features_, train_labels_, test_labels_ = train_test_split(\n",
    "    input_data_x_, output_label_y_, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features__, test_features__, train_labels__, test_labels__ = train_test_split(\n",
    "    input_data_x__, output_label_y__, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, MaxPooling3D, AveragePooling3D, Dense, Dropout, GlobalAveragePooling3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the conv3d_bn function as above\n",
    "def conv3d_bn(x, filters, num_row, num_col, num_depth, strides=(1,1,1), padding='same', name=None):\n",
    "    x = layers.Conv3D(\n",
    "        filters, \n",
    "        kernel_size=(num_depth, num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=name+'_conv3d')(x)\n",
    "    x = layers.BatchNormalization(axis=-1, scale=False, name=name+'_bn')(x)\n",
    "    x = layers.Activation('relu', name=name+'_relu')(x)\n",
    "    return x\n",
    "\n",
    "# Define the Inception_Inflated3d function as updated above\n",
    "def Inception_Inflated3d(include_top=True,\n",
    "                         weights=None,\n",
    "                         input_tensor=None,\n",
    "                         input_shape=None,\n",
    "                         dropout_prob=0.0,\n",
    "                         endpoint_logit=True,\n",
    "                         classes=400):\n",
    "    \n",
    "    img_input = Input(shape=input_shape)\n",
    "    channel_axis = -1  # channels_last\n",
    "\n",
    "    x = conv3d_bn(img_input, 64, 7, 7, 7, strides=(2, 2, 2), padding='same', name='Conv3d_1a_7x7')\n",
    "    # Downsampling (spatial only)\n",
    "    x = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same', name='MaxPool2d_2a_3x3')(x)\n",
    "    x = conv3d_bn(x, 64, 1, 1, 1, strides=(1, 1, 1), padding='same', name='Conv3d_2b_1x1')\n",
    "    x = conv3d_bn(x, 192, 3, 3, 3, strides=(1, 1, 1), padding='same', name='Conv3d_2c_3x3')\n",
    "    # Downsampling (spatial only)\n",
    "    x = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same', name='MaxPool2d_3a_3x3')(x)\n",
    "    # Mixed 3b\n",
    "    branch_0 = conv3d_bn(x, 64, 1, 1, 1, padding='same', name='Conv3d_3b_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 96, 1, 1, 1, padding='same', name='Conv3d_3b_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 128, 3, 3, 3, padding='same', name='Conv3d_3b_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 16, 1, 1, 1, padding='same', name='Conv3d_3b_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 32, 3, 3, 3, padding='same', name='Conv3d_3b_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_3b_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 32, 1, 1, 1, padding='same', name='Conv3d_3b_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_3b')\n",
    "    # Mixed 3c\n",
    "    branch_0 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_3c_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_3c_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 192, 3, 3, 3, padding='same', name='Conv3d_3c_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_3c_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 96, 3, 3, 3, padding='same', name='Conv3d_3c_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_3c_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_3c_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_3c')\n",
    "    # Downsampling (spatial and temporal)\n",
    "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), padding='same', name='MaxPool2d_4a_3x3')(x)\n",
    "    # Mixed 4b\n",
    "    branch_0 = conv3d_bn(x, 192, 1, 1, 1, padding='same', name='Conv3d_4b_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 96, 1, 1, 1, padding='same', name='Conv3d_4b_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 208, 3, 3, 3, padding='same', name='Conv3d_4b_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 16, 1, 1, 1, padding='same', name='Conv3d_4b_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 48, 3, 3, 3, padding='same', name='Conv3d_4b_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4b_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4b_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_4b')\n",
    "    # Mixed 4c\n",
    "    branch_0 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_4c_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 112, 1, 1, 1, padding='same', name='Conv3d_4c_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 224, 3, 3, 3, padding='same', name='Conv3d_4c_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 24, 1, 1, 1, padding='same', name='Conv3d_4c_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4c_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4c_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4c_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_4c')\n",
    "    # Mixed 4d\n",
    "    branch_0 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_4d_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_4d_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 256, 3, 3, 3, padding='same', name='Conv3d_4d_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 24, 1, 1, 1, padding='same', name='Conv3d_4d_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4d_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4d_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4d_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_4d')\n",
    "    # Mixed 4e\n",
    "    branch_0 = conv3d_bn(x, 112, 1, 1, 1, padding='same', name='Conv3d_4e_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 144, 1, 1, 1, padding='same', name='Conv3d_4e_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 288, 3, 3, 3, padding='same', name='Conv3d_4e_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_4e_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4e_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4e_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4e_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_4e')\n",
    "    # Mixed 4f\n",
    "    branch_0 = conv3d_bn(x, 256, 1, 1, 1, padding='same', name='Conv3d_4f_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_4f_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 320, 3, 3, 3, padding='same', name='Conv3d_4f_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_4f_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_4f_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4f_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_4f_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_4f')\n",
    "    # Downsampling (spatial and temporal)\n",
    "    x = MaxPooling3D((2, 2, 2), strides=(2, 2, 2), padding='same', name='MaxPool2d_5a_2x2')(x)\n",
    "    # Mixed 5b\n",
    "    branch_0 = conv3d_bn(x, 256, 1, 1, 1, padding='same', name='Conv3d_5b_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_5b_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 320, 3, 3, 3, padding='same', name='Conv3d_5b_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_5b_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_5b_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_5b_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_5b_3b_1x1')\n",
    "    x = layers.concatenate(\n",
    "        [branch_0, branch_1, branch_2, branch_3],\n",
    "        axis=channel_axis,\n",
    "        name='Mixed_5b')\n",
    "    # Mixed 5c\n",
    "    branch_0 = conv3d_bn(x, 384, 1, 1, 1, padding='same', name='Conv3d_5c_0a_1x1')\n",
    "    branch_1 = conv3d_bn(x, 192, 1, 1, 1, padding='same', name='Conv3d_5c_1a_1x1')\n",
    "    branch_1 = conv3d_bn(branch_1, 384, 3, 3, 3, padding='same', name='Conv3d_5c_1b_3x3')\n",
    "    branch_2 = conv3d_bn(x, 48, 1, 1, 1, padding='same', name='Conv3d_5c_2a_1x1')\n",
    "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_5c_2b_3x3')\n",
    "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_5c_3a_3x3')(x)\n",
    "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_5c_3b_1x1')\n",
    "    \n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling3D(name='global_avg_pool')(x)\n",
    "        if dropout_prob > 0:\n",
    "            x = Dropout(dropout_prob, name='dropout')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    else:\n",
    "        x = GlobalAveragePooling3D(name='global_avg_pool')(x)\n",
    "    \n",
    "    model = Model(img_input, x, name='i3d_inception')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3719, 10, 300, 300, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_Incep = np.repeat(train_features[:, np.newaxis, :, :, :], 10, axis=1)\n",
    "train_features_Incep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 16:07:15.418421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
      "2024-10-30 16:07:15.906105: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-30 16:07:16.995218: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ce2bf60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-30 16:07:16.995275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-10-30 16:07:16.995291: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-10-30 16:07:16.995303: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-10-30 16:07:16.995314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-10-30 16:07:17.000547: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-30 16:07:17.052152: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-30 16:07:17.094179: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465/465 [==============================] - 155s 293ms/step - loss: 0.5056 - accuracy: 0.7642\n",
      "Epoch 2/10\n",
      "465/465 [==============================] - 134s 289ms/step - loss: 0.4285 - accuracy: 0.8099\n",
      "Epoch 3/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.3998 - accuracy: 0.8223\n",
      "Epoch 4/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.3468 - accuracy: 0.8486\n",
      "Epoch 5/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.3324 - accuracy: 0.8658\n",
      "Epoch 6/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.3387 - accuracy: 0.8564\n",
      "Epoch 7/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.3313 - accuracy: 0.8604\n",
      "Epoch 8/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.2922 - accuracy: 0.8849\n",
      "Epoch 9/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.2633 - accuracy: 0.8935\n",
      "Epoch 10/10\n",
      "465/465 [==============================] - 134s 288ms/step - loss: 0.2588 - accuracy: 0.8978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f48b0398e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Inception_Inflated3d(include_top=True, input_shape=(10, 300, 300, 1), classes=2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_features_Incep, train_labels, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('inception_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION OF SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs\n",
    "initjs()\n",
    "custom_objects = None\n",
    "model_mse = tf.keras.models.load_model('cvd_cnn_1.h5', custom_objects=custom_objects, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_images_as_lists = []\n",
    "image_shape = (300, 300, 1)\n",
    "\n",
    "for image in test_features:\n",
    "    # Ensure the image has the correct shape\n",
    "    if image.shape != image_shape:\n",
    "        raise ValueError(f\"Image shape {image.shape} does not match expected shape {image_shape}\")\n",
    "\n",
    "    # Initialize the masker for a single image\n",
    "    masker = shap.maskers.Image(\"blur(32,32)\", image_shape)\n",
    "\n",
    "    # Apply the masker to each image\n",
    "    masked_image_tuple = masker(image, np.zeros(image.shape[:-1]))  # Adjust the mask as needed\n",
    "    \n",
    "    # Extract the first element of the tuple (which is the masked image)\n",
    "    masked_image = masked_image_tuple[0]\n",
    "    \n",
    "    # Convert the masked image to a list and store it\n",
    "    masked_images_as_lists.append(masked_image.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explainer for our both negative and positive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain = shap.GradientExplainer(model_mse,test_features)\n",
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explainer for our positive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain1 = shap.GradientExplainer(model_mse,test_features__)\n",
    "explain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explainer for our negative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain0 = shap.GradientExplainer(model_mse,test_features_)\n",
    "explain0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for the Images on the test features both positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    imshow(test_features[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute shap values for both positive and negative test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explain.shap_values(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute shap values for positive test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values1 = explain1.shap_values(test_features__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute shap values for negative test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values0 = explain1.shap_values(test_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values0[:5], test_features_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot for both postive and negative test sammple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot for positive test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values0[:5], test_features__[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(my_shap_values, my_features,aspect=0.9, hspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Explainer for Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_incept = tf.keras.models.load_model('inception_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_incept.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainInception = shap.GradientExplainer(model_incept)\n",
    "explainInception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_incept = explainInception.shap_values(test_features_Incep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_incept[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove non important regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_shap = shap_values_incept[:, :, :, :, 0]\n",
    "my_new_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values), 95)\n",
    "save_path = \"dataset_important_features/Inception/SHAP\" \n",
    "process_and_save_images(test_features, my_new_shap, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrain on important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/Inception/Shap/'\n",
    "new_prefix = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_shap_features_Incept_df =  processImages(new_path,1)\n",
    "ct_covid_shap_features_Incept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_shap_features_Incept_df]\n",
    "cvd_imgs_incept_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_incept = cvd_imgs_incept_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_incept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_incept_x = cvd_imgs_dataset_shap_incept.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_incept['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_incept['class_label'])\n",
    "cvd_imgs_dataset_shap_incept\n",
    "cvd_imgs_dataset_shap_incept = pd.get_dummies(cvd_imgs_dataset_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_incept_y = np.array(cvd_imgs_dataset_shap[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_incept_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_incept_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_incept_shap, test_features_incept_shap, train_labels_incept_shap, test_labels_incept_shap = train_test_split(\n",
    "    input_data_shap_incept_x, output_label_shap_incept_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_incept_import = Inception_Inflated3d(include_top=True, input_shape=(10, 300, 300, 1), classes=2)\n",
    "\n",
    "# Compile the model\n",
    "model_incept_import.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_incept_import.fit(train_features_incept_shap, train_labels_incept_shap, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply LIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_Incep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_incept_import.predict(images)\n",
    "\n",
    "single_image = train_features_Incep[0] \n",
    "single_image.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_incept = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import gaussian\n",
    "import os\n",
    "\n",
    "def remove_non_important_features(images, masks, method='mean', blur_sigma=5, save=False, save_path='dataset_important_features'):\n",
    "    if not isinstance(images, (list, np.ndarray)):\n",
    "        raise TypeError(\"Images should be a list or numpy array of image arrays.\")\n",
    "        \n",
    "    if not isinstance(masks, (list, np.ndarray)):\n",
    "        raise TypeError(\"Masks should be a list or numpy array of mask arrays.\")\n",
    "        \n",
    "    if len(images) != len(masks):\n",
    "        raise ValueError(\"The number of images and masks must be the same.\")\n",
    "    \n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for idx, (image, mask) in enumerate(zip(images, masks)):\n",
    "        if image.shape[:2] != mask.shape:\n",
    "            raise ValueError(f\"Image and mask at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        processed_image = image.copy()\n",
    "        \n",
    "        if method == 'mean':\n",
    "            if image.ndim == 3:  # Color image\n",
    "                mean_value = image.mean(axis=(0,1), keepdims=True)\n",
    "            else:  # Grayscale image\n",
    "                mean_value = image.mean()\n",
    "            processed_image[mask == 0] = mean_value\n",
    "        \n",
    "        elif method == 'zero':\n",
    "            processed_image[mask == 0] = 0\n",
    "        \n",
    "        elif method == 'blur':\n",
    "            blurred_image = gaussian(processed_image, sigma=blur_sigma, multichannel=True)\n",
    "            processed_image[mask == 0] = blurred_image[mask == 0]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid method chosen. Available methods: 'mean', 'zero', 'blur'.\")\n",
    "        \n",
    "        processed_images.append(processed_image)\n",
    "        \n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(processed_image)\n",
    "            imsave(os.path.join(save_path, f'processed_image_{idx}.png'), image_to_save)\n",
    "            print(f\"Saved: {os.path.join(save_path, f'processed_image_{idx}.png')}\")\n",
    "    \n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = remove_non_important_features(\n",
    "    images=single_image,\n",
    "    masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/Inception/LIME'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/Inception/LIME/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_Lime_features_Incept_df =  processImages(new_path,1)\n",
    "ct_covid_Lime_features_Incept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_Lime_features_Incept_df]\n",
    "cvd_imgs_incept_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime_incept = cvd_imgs_incept_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime_incept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_incept_x = cvd_imgs_dataset_lime_incept.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_incept['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_incept['class_label'])\n",
    "cvd_imgs_dataset_lime_incept\n",
    "cvd_imgs_dataset_lime_incept = pd.get_dummies(cvd_imgs_dataset_lime_incept, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_incept_y = np.array(cvd_imgs_dataset_lime_incept[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_incept_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_incept_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_incept_lime, test_features_incept_lime, train_labels_incept_lime, test_labels_incept_lime = train_test_split(\n",
    "    input_data_lime_incept_x, output_label_lime_incept_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_incept_import = Inception_Inflated3d(include_top=True, input_shape=(10, 300, 300, 1), classes=2)\n",
    "\n",
    "# Compile the model\n",
    "model_incept_import.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_incept_import.fit(train_features_incept_lime, train_labels_incept_lime, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply Interception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]\n",
    "len(masker_shap_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]\n",
    "len(test_features_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "def find_intersection_and_save(images, shap_masks, lime_masks, save=False, save_path='dataset_intersection'):\n",
    "    \"\"\"\n",
    "    Finds the intersection between SHAP and LIME masks and applies it to images.\n",
    "    Highlights the intersection regions and overlays them on the original image.\n",
    "\n",
    "    Args:\n",
    "        images (list or np.ndarray): List or array of images. Each image should be a numpy array.\n",
    "        shap_masks (list or np.ndarray): SHAP binary masks for important features.\n",
    "        lime_masks (list or np.ndarray): LIME binary masks for important features.\n",
    "        save (bool): Whether to save the images with important features.\n",
    "        save_path (str): Directory path to save the images if save is True.\n",
    "\n",
    "    Returns:\n",
    "        intersection_images (list): List of images with intersection regions highlighted.\n",
    "    \"\"\"\n",
    "    if len(images) != len(shap_masks) or len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images, SHAP masks, and LIME masks must be the same.\")\n",
    "    \n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    intersection_images = []\n",
    "\n",
    "    for idx, (image, shap_mask, lime_mask) in enumerate(zip(images, shap_masks, lime_masks)):\n",
    "        # Ensure image and masks have matching dimensions\n",
    "        if image.shape[:2] != shap_mask.shape or image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and masks at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        # Ensure masks are binary and of boolean type\n",
    "        shap_mask = shap_mask.astype(bool)\n",
    "        lime_mask = lime_mask.astype(bool)\n",
    "        \n",
    "        # Find the intersection of SHAP and LIME masks\n",
    "        intersection_mask = np.logical_and(shap_mask, lime_mask)\n",
    "        \n",
    "        # Create a copy of the image to process\n",
    "        intersection_image = np.copy(image)\n",
    "\n",
    "        # Ensure the image is in RGB format for highlighting\n",
    "        if intersection_image.ndim == 2:  # For grayscale images\n",
    "            intersection_image = np.stack([intersection_image] * 3, axis=-1)  # Convert to RGB\n",
    "\n",
    "        # Create an overlay image with the highlighted regions\n",
    "        overlay_image = np.copy(intersection_image)\n",
    "        highlight_color = [255, 0, 0]  # Red color\n",
    "\n",
    "        # Apply the highlight color to the intersection areas\n",
    "        overlay_image[intersection_mask] = highlight_color\n",
    "\n",
    "        # Combine original image and overlay image\n",
    "        final_image = np.where(intersection_mask[:, :, np.newaxis], overlay_image, intersection_image)\n",
    "\n",
    "        intersection_images.append(final_image)\n",
    "        \n",
    "        if save:\n",
    "            # Convert image to uint8 before saving\n",
    "            image_to_save = img_as_ubyte(final_image)\n",
    "\n",
    "            # Ensure image is either 2D or 3D\n",
    "            if len(image_to_save.shape) not in [2, 3]:\n",
    "                raise ValueError(f\"Unexpected image shape for saving: {image_to_save.shape}\")\n",
    "\n",
    "            # Save the image\n",
    "            imsave(os.path.join(save_path, f'intersection_image_{idx}.png'), image_to_save)\n",
    "    \n",
    "    return intersection_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = find_intersection_and_save(test_features_[:300], masker_shap_[:300], train_features_incept_lime, save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/inception/intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/inception/intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((2473,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    imshow(input_data_intersection_x[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_incept_import = Inception_Inflated3d(include_top=True, input_shape=(10, 300, 300, 1), classes=2)\n",
    "\n",
    "# Compile the model\n",
    "model_incept_import.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_incept_import.fit(train_features_intersection, train_labels_intersection, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19(input_shape=None, classes=5, use_soft=True):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv1', kernel_initializer=\"he_normal\")(img_input)\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Classification block\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu', name='fc2')(x)\n",
    "\n",
    "    # Output layer\n",
    "    if use_soft:\n",
    "        x = Dense(classes, activation=\"softmax\", name='predictions')(x)\n",
    "    else:\n",
    "        x = Dense(classes, activation=\"linear\", name=\"Z_4\")(x)\n",
    "\n",
    "    modelss = models.Model(img_input, x, name='vgg19')\n",
    "\n",
    "    return modelss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.fit(train_features, train_labels, epochs=20, validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.save('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Explainer for VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG19 = tf.keras.models.load_model('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG19 = shap.GradientExplainer(model_VGG19,train_features)\n",
    "explainVGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_ = explainVGG19.shap_values(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mask out the Important regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_and_save_images(images, shap_values, threshold, output_folder):\n",
    "    \"\"\"\n",
    "    Process images by removing non-important features based on SHAP values,\n",
    "    change the background to white, and apply a red color to non-important features.\n",
    "\n",
    "    Parameters:\n",
    "    - images: numpy array of shape (num_images, height, width, channels)\n",
    "    - shap_values: numpy array of shape (num_images, height, width, channels)\n",
    "    - threshold: importance threshold to decide which features are important\n",
    "    - output_folder: directory to save the processed images\n",
    "    \"\"\"\n",
    "\n",
    "    # Create masks based on SHAP values and threshold\n",
    "    important_features_mask = np.abs(shap_values) > threshold\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Process and save each image\n",
    "    for idx, (img, mask) in enumerate(zip(images, important_features_mask)):\n",
    "        # Ensure image and mask are of the same shape\n",
    "        if img.shape != mask.shape:\n",
    "            raise ValueError(f\"Image shape {img.shape} and mask shape {mask.shape} do not match.\")\n",
    "        \n",
    "        # Create a white background\n",
    "        white_background = np.ones_like(img) * 255\n",
    "\n",
    "        # Apply mask to keep important features and set the rest to white\n",
    "        processed_img = np.where(mask, img, white_background)\n",
    "\n",
    "        # Convert non-important features (where mask is False) to red (for RGB images)\n",
    "        if img.shape[-1] == 3:  # If the image has 3 channels (colored image)\n",
    "            red_mask = np.zeros_like(img)\n",
    "            red_mask[..., 2] = 255  # Red color in RGB\n",
    "\n",
    "            # Ensure to only modify non-important features (where mask is False)\n",
    "            processed_img[~mask] = red_mask[~mask]\n",
    "\n",
    "        # Ensure the image is within the range 0-255 for saving\n",
    "        processed_img = processed_img.astype(np.uint8)\n",
    "\n",
    "        # Save the image\n",
    "        output_file = os.path.join(output_folder, f'processed_image_{idx}.png')\n",
    "        cv2.imwrite(output_file, processed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_shap = shap_values_[:, :, :, :, 0]\n",
    "my_model_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values_), 95)\n",
    "save_path = \"dataset_important_features/VGG19/SHAP\"  # Replace with your directory path\n",
    "process_and_save_images(test_features, my_model_shap, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Retrain the model on important images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/SHAP/'\n",
    "new_prefix = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_shap_features_VGG19_df =  processImages(new_path,1)\n",
    "ct_covid_shap_features_VGG19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_shap_features_VGG19_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_VGG19 = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap_VGG19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_VGG19_x = cvd_imgs_dataset_shap_VGG19.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_VGG19['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_VGG19['class_label'])\n",
    "cvd_imgs_dataset_shap_VGG19\n",
    "cvd_imgs_dataset_shap_VGG19 = pd.get_dummies(cvd_imgs_dataset_shap_VGG19, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_VGG19_y = np.array(cvd_imgs_dataset_shap_VGG19[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_VGG19_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_VGG19_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap_VGG19, test_features_shap_VGG19, train_labels_shap_VGG19, test_labels_shap_VGG19 = train_test_split(\n",
    "    input_data_shap_VGG19_x, output_label_shap_VGG19_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_shap_VGG19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def VGG19(input_shape=None, classes=5, use_soft=True):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv1', kernel_initializer=\"he_normal\")(img_input)\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Classification block\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu', name='fc2')(x)\n",
    "\n",
    "    # Output layer\n",
    "    if use_soft:\n",
    "        x = Dense(classes, activation=\"softmax\", name='predictions')(x)\n",
    "    else:\n",
    "        x = Dense(classes, activation=\"linear\", name=\"Z_4\")(x)\n",
    "\n",
    "    model_vgg19 = models.Model(img_input, x, name='vgg19')\n",
    "\n",
    "    return model_vgg19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "model_vgg19.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_shap_VGG19, train_labels_shap_VGG19, epochs=10, validation_data=(test_features_shap_VGG19, test_labels_shap_VGG19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Apply LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_vgg19.predict(images)\n",
    "\n",
    "single_image = train_features_Incep[0] \n",
    "single_image.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask_VGG16 = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_VGG19 = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = remove_non_important_features(\n",
    "    images=single_image,\n",
    "    masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG19/LIME'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/LIME/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_Lime_features_VGG19_df =  processImages(new_path,1)\n",
    "ct_covid_Lime_features_VGG19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_Lime_features_VGG19_df]\n",
    "cvd_imgs_VGG19_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime_VGG19 = cvd_imgs_VGG19_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime_VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_VGG19_x = cvd_imgs_dataset_lime_VGG19.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_VGG19['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_VGG19['class_label'])\n",
    "cvd_imgs_dataset_lime_VGG19\n",
    "cvd_imgs_dataset_lime_VGG19 = pd.get_dummies(cvd_imgs_dataset_lime_VGG19, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_VGG19_y = np.array(cvd_imgs_dataset_lime_VGG19[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_VGG19_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_VGG19_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_VGG19_lime, test_features_VGG19_lime, train_labels_VGG19_lime, test_labels_VGG19_lime = train_test_split(\n",
    "    input_data_lime_VGG19_x, output_label_lime_VGG19_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "model_vgg19.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_VGG19_lime, train_labels_VGG19_lime, epochs=10, validation_data=(test_features_VGG19_lime, test_labels_VGG19_lime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]\n",
    "len(masker_shap_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]\n",
    "len(test_features_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "def find_intersection_and_save(images, shap_masks, lime_masks, save=False, save_path='dataset_intersection'):\n",
    "    \"\"\"\n",
    "    Finds the intersection between SHAP and LIME masks and applies it to images.\n",
    "    Highlights the intersection regions and overlays them on the original image.\n",
    "\n",
    "    Args:\n",
    "        images (list or np.ndarray): List or array of images. Each image should be a numpy array.\n",
    "        shap_masks (list or np.ndarray): SHAP binary masks for important features.\n",
    "        lime_masks (list or np.ndarray): LIME binary masks for important features.\n",
    "        save (bool): Whether to save the images with important features.\n",
    "        save_path (str): Directory path to save the images if save is True.\n",
    "\n",
    "    Returns:\n",
    "        intersection_images (list): List of images with intersection regions highlighted.\n",
    "    \"\"\"\n",
    "    if len(images) != len(shap_masks) or len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images, SHAP masks, and LIME masks must be the same.\")\n",
    "    \n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    intersection_images = []\n",
    "\n",
    "    for idx, (image, shap_mask, lime_mask) in enumerate(zip(images, shap_masks, lime_masks)):\n",
    "        # Ensure image and masks have matching dimensions\n",
    "        if image.shape[:2] != shap_mask.shape or image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and masks at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        # Ensure masks are binary and of boolean type\n",
    "        shap_mask = shap_mask.astype(bool)\n",
    "        lime_mask = lime_mask.astype(bool)\n",
    "        \n",
    "        # Find the intersection of SHAP and LIME masks\n",
    "        intersection_mask = np.logical_and(shap_mask, lime_mask)\n",
    "        \n",
    "        # Create a copy of the image to process\n",
    "        intersection_image = np.copy(image)\n",
    "\n",
    "        # Ensure the image is in RGB format for highlighting\n",
    "        if intersection_image.ndim == 2:  # For grayscale images\n",
    "            intersection_image = np.stack([intersection_image] * 3, axis=-1)  # Convert to RGB\n",
    "\n",
    "        # Create an overlay image with the highlighted regions\n",
    "        overlay_image = np.copy(intersection_image)\n",
    "        highlight_color = [255, 0, 0]  # Red color\n",
    "\n",
    "        # Apply the highlight color to the intersection areas\n",
    "        overlay_image[intersection_mask] = highlight_color\n",
    "\n",
    "        # Combine original image and overlay image\n",
    "        final_image = np.where(intersection_mask[:, :, np.newaxis], overlay_image, intersection_image)\n",
    "\n",
    "        intersection_images.append(final_image)\n",
    "        \n",
    "        if save:\n",
    "            # Convert image to uint8 before saving\n",
    "            image_to_save = img_as_ubyte(final_image)\n",
    "\n",
    "            # Ensure image is either 2D or 3D\n",
    "            if len(image_to_save.shape) not in [2, 3]:\n",
    "                raise ValueError(f\"Unexpected image shape for saving: {image_to_save.shape}\")\n",
    "\n",
    "            # Save the image\n",
    "            imsave(os.path.join(save_path, f'intersection_image_{idx}.png'), image_to_save)\n",
    "    \n",
    "    return intersection_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = find_intersection_and_save(test_features_[:300], masker_shap_[:300], train_labels_VGG19_lime, save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/Intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((2473,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_intersection, train_labels_intersection, epochs=10, validation_data=(test_features_intersection, test_labels_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def VGG16():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1), input_shape=(300, 300, 1)))  # Input shape corrected\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(2, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(2, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='inter'))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', name='fc2'))\n",
    "    model.add(Dense(2, activation='softmax'))  # Updated to 2 classes for binary classification\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features, train_labels, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.save('VGG16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16 = tf.keras.models.load_model('VGG16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG16 = shap.GradientExplainer(model_VGG16,train_features)\n",
    "explainVGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_valuesVGG16 = explainVGG16.shap_values(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_valuesVGG16[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_shap_ = shap_valuesVGG16[:, :, :, :, 0]\n",
    "my_new_shap_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values_), 95)\n",
    "save_path = \"dataset_important_features/VGG16/SHAP\"  # Replace with your directory path\n",
    "process_and_save_images(test_features, my_new_shap_, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/SHAP/'\n",
    "new_prefix = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_shap_features_VGG16_df =  processImages(new_path,1)\n",
    "ct_covid_shap_features_VGG16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_shap_features_VGG16_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_VGG16 = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_VGG16_x = cvd_imgs_dataset_shap_VGG16.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_VGG16['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_VGG16['class_label'])\n",
    "cvd_imgs_dataset_shap_VGG16\n",
    "cvd_imgs_dataset_shap_VGG16 = pd.get_dummies(cvd_imgs_dataset_shap_VGG16, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_VGG16_y = np.array(cvd_imgs_dataset_shap_VGG16[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_VGG16_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_VGG16_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap_VGG16, test_features_shap_VGG16, train_labels_shap_VGG16, test_labels_shap_VGG16 = train_test_split(\n",
    "    input_data_shap_VGG16_x, output_label_shap_VGG16_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features_shap_VGG16, train_labels_shap_VGG16, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_VGG16.predict(images)\n",
    "\n",
    "single_image = test_features[0] \n",
    "single_image.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks__ = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = remove_non_important_features(\n",
    "    images=single_image,\n",
    "    masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG16/LIME'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/LIME/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_Lime_features_VGG19_df =  processImages(new_path,1)\n",
    "ct_covid_Lime_features_VGG19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_VGG16_x = cvd_imgs_dataset_lime_VGG16.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_VGG16['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_VGG16['class_label'])\n",
    "cvd_imgs_dataset_lime_VGG16\n",
    "cvd_imgs_dataset_lime_VGG16 = pd.get_dummies(cvd_imgs_dataset_lime_VGG16, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_VGG16_y = np.array(cvd_imgs_dataset_lime_VGG16[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_VGG16_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_VGG16_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_VGG16_lime, test_features_VGG16_lime, train_labels_VGG16_lime, test_labels_VGG16_lime = train_test_split(\n",
    "    input_data_lime_VGG16_x, output_label_lime_VGG16_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features_VGG16_lime, train_labels_VGG16_lime, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding the Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]\n",
    "len(masker_shap_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]\n",
    "len(test_features_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = find_intersection_and_save(test_features_[:300], masker_shap_[:300], train_labels_VGG19_lime, save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train on the intersection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((2473,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inter = VGG16()\n",
    "model_inter.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_inter.fit(train_features_intersection, train_labels_intersection, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS FOR LIME ON OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Positive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features__[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Negative test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_mse.predict(images)\n",
    "\n",
    "single_image = test_features[0] \n",
    "single_image.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LIME for Positive test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_mse.predict(images)\n",
    "\n",
    "single_image__ = test_features__[0] \n",
    "single_image__.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer__ = lime_image.LimeImageExplainer()\n",
    "explanations__ = []\n",
    "masks__ = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image__ = test_features__[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation__ = explainer__.explain_instance(single_image__,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations__.append(explanation__)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp__, mask__ = explanation__.get_image_and_mask(explanation__.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks__.append(mask__)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features__[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image__.squeeze(), mask__), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(single_image__[1].shape), len(mask__[1].shape))  # Ensure they match in length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Sample images and masks (for demonstration)\n",
    "# Replace these with actual images and masks from your dataset\n",
    "#images = [data.astronaut(), data.coffee()]  # Example images\n",
    "#masks = [np.random.randint(0, 2, size=image.shape[:2]) for image in images]  # Simulated LIME masks\n",
    "\n",
    "resized_masks = [resize_mask(mask__, image.shape) for mask__, image in zip(masks__, single_image__)]\n",
    "\n",
    "# Apply the function\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image__,\n",
    "    lime_masks=resized_masks,\n",
    "    method='blur',           # Options: 'blur', 'mean', 'zero'\n",
    "    blur_sigma=3,            # Adjust blur level if using the 'blur' method\n",
    "    save=True,               # Set to True to save the images\n",
    "    save_path='dataset_important_features/Lime/Positive'  # Directory to save images\n",
    ")\n",
    "\n",
    "# Display the result (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, img in enumerate(processed_images):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Processed Image {i}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Image shape: {single_image__[0].shape}\")\n",
    "print(f\"Original Mask shape: {resized_masks[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image__[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks__[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "#resized_masks = [resize_mask(mask__, image.shape) for mask__, image in zip(masks__, single_image__)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image__,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/Lime/Positive'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LIME for Negative test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_mse.predict(images)\n",
    "\n",
    "single_image_ = test_features_[0] \n",
    "single_image_.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer_ = lime_image.LimeImageExplainer()\n",
    "explanations_ = []\n",
    "masks_ = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image_ = test_features_[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation_ = explainer_.explain_instance(single_image_,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations_.append(explanation_)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp_, mask_ = explanation_.get_image_and_mask(explanation_.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks_.append(mask_)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features_[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image_.squeeze(), mask_), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Image shape: {single_image_[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks_[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image__[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks__[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask_, image.shape) for mask_, image in zip(masks_, single_image_)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image_,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/Lime/Negative'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path___ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/Lime/Positive/'\n",
    "new_prefix__ = 'ct_covid'\n",
    "\n",
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/Lime/Negative/'\n",
    "new_prefix__ = 'non_ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_lime =  processImages(new_path___,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_lime =  processImages(new_path__,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_lime = [ct_noncovid_features_df_lime, ct_covid_features_df_lime]\n",
    "cvd_imgs_dataset_lime = pd.concat(cvd_imgs_lime)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime_ = cvd_imgs_dataset_lime.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_lime_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_x = cvd_imgs_dataset_lime_.iloc[:,:-1].to_numpy().reshape((600,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_['class_label'])\n",
    "cvd_imgs_dataset_lime_\n",
    "cvd_imgs_dataset_lime_ = pd.get_dummies(cvd_imgs_dataset_lime_, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_y = np.array(cvd_imgs_dataset_lime_[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_lime, test_features_lime, train_labels_lime, test_labels_lime = train_test_split(\n",
    "    input_data_lime_x, output_label_lime_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "    '''\n",
    "    CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "        model            = CNN Model\n",
    "        num_hl           = Number of hidden layers\n",
    "        hl_list          = List of hidden layer units\n",
    "        hl_conv_activation = Activation function for convolutional layers\n",
    "        ol_activation    = Activation function for the output layer\n",
    "        dropout_val      = List of dropout values for hidden layers\n",
    "        inputShape       = Shape of the input data (e.g., (300, 300, 1))\n",
    "        filters_         = Number of filters for Conv layers\n",
    "        kernel_size_     = Size of the kernel (height, width)\n",
    "        stride_poolSize  = Stride and pool size (same value for both)\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    \n",
    "    # Augment data (assuming data_augmentation is predefined)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    # Convolutional Layer 1 with Max-Pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(augmented)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    # Convolutional Layer 2 with Max-Pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_2_max_pool)\n",
    "    \n",
    "    # Flattening the output of the last convolutional block\n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    # Adding Dense and Dropout Layers\n",
    "    dense_layers = flatten_layer\n",
    "    for i in range(num_hl):\n",
    "        dense_layers = tf.keras.layers.Dense(units=hl_list[i], activation=hl_conv_activation)(dense_layers)\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_val[i])(dense_layers)\n",
    "    \n",
    "    # Output Layer\n",
    "    logits = tf.keras.layers.Dense(units=2, activation=ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def compile_model(model, loss_, learningRate, metrics_):\n",
    "    '''\n",
    "    def compile_model(model, loss_, learningRate, metrics_):\n",
    "    model: the model to compile\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: list of metrics to track (e.g., ['accuracy'])\n",
    "    '''\n",
    "    # Use the legacy Adam optimizer if you're on an M1/M2 Mac\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate, weight_decay=0.001)\n",
    "\n",
    "    # Alternatively, if needed, you can enable eager execution\n",
    "    # tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_, metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "  '''\n",
    "  def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "  '''\n",
    "  history = model.fit(\n",
    "    train_features_lime, \n",
    "    train_labels_lime,\n",
    "    validation_split = val_split_size,\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluateModel_loss(history):\n",
    "    print()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training loss','Validation loss'], loc = 'upper left')\n",
    "    plt.savefig(\"Training_validation_loss_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    print()\n",
    "def evaluateModel_accuracy(history):\n",
    "    print()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training Accuracy','Validation Accuracy'], loc = 'upper left')\n",
    "    #plt.show()\n",
    "    plt.savefig(\"Training_validation_accuracy_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()\n",
    "\n",
    "print(CNN_Mod.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 200\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Normalization(),\n",
    "        tf.keras.layers.Resizing(image_size, image_size),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(train_features_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "cnn_1  = tf.keras.models.Sequential()\n",
    "cnn_1  = CNN_Mod(cnn_1, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_1  = compile_model(cnn_1,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "history = buildModel(cnn_1,0.20,128,100,10, \"val_loss\", \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_ = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters import gaussian\n",
    "import os\n",
    "\n",
    "def apply_lime_mask(images, lime_masks, method='blur', blur_sigma=5, save=False, save_path='dataset_lime_filtered'):\n",
    "    \"\"\"\n",
    "    Removes unimportant regions from images based on LIME masks.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list or np.ndarray): List or array of original images.\n",
    "    - lime_masks (list or np.ndarray): List or array of LIME masks (1 for important regions, 0 for unimportant).\n",
    "    - method (str): Method to fill unimportant regions ('blur', 'mean', or 'zero').\n",
    "    - blur_sigma (int): Sigma for Gaussian blur if 'blur' method is selected.\n",
    "    - save (bool): Whether to save processed images.\n",
    "    - save_path (str): Directory path to save images if save is True.\n",
    "\n",
    "    Returns:\n",
    "    - processed_images (list): List of images with unimportant regions removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify input types\n",
    "    if not isinstance(images, (list, np.ndarray)):\n",
    "        raise TypeError(\"Images should be a list or numpy array of image arrays.\")\n",
    "        \n",
    "    if not isinstance(lime_masks, (list, np.ndarray)):\n",
    "        raise TypeError(\"LIME masks should be a list or numpy array of mask arrays.\")\n",
    "        \n",
    "    if len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images and LIME masks must be the same.\")\n",
    "    \n",
    "    # Create save directory if needed\n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for idx, (image, lime_mask) in enumerate(zip(images, lime_masks)):\n",
    "        # Ensure image and mask dimensions match\n",
    "        if image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and LIME mask at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        # Copy the image for processing\n",
    "        processed_image = image.copy()\n",
    "        \n",
    "        # Apply the chosen method to unimportant regions (where lime_mask == 0)\n",
    "        if method == 'blur':\n",
    "            blurred_image = gaussian(processed_image, sigma=blur_sigma)\n",
    "            processed_image[lime_mask == 0] = blurred_image[lime_mask == 0]\n",
    "        \n",
    "        elif method == 'mean':\n",
    "            mean_value = image.mean(axis=(0, 1), keepdims=True) if image.ndim == 3 else image.mean()\n",
    "            processed_image[lime_mask == 0] = mean_value\n",
    "        \n",
    "        elif method == 'zero':\n",
    "            processed_image[lime_mask == 0] = 0\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid method chosen. Available methods: 'blur', 'mean', 'zero'.\")\n",
    "        \n",
    "        # Append processed image to the list\n",
    "        processed_images.append(processed_image)\n",
    "        \n",
    "        # Save processed image if required\n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(processed_image)\n",
    "            imsave(os.path.join(save_path, f'lime_filtered_image_{idx}.png'), image_to_save, check_contrast=False)\n",
    "            print(f\"Saved: {os.path.join(save_path, f'lime_filtered_image_{idx}.png')}\")\n",
    "    \n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import felzenszwalb\n",
    "\n",
    "\n",
    "def predict_fn(images):\n",
    "    return model_mse.predict(images)\n",
    "\n",
    "num_images_to_explain = 300\n",
    "# Initialize LIME explainer\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "# Generate explanations and masks for each image\n",
    "masks = []\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    # Get mask for the top predicted label\n",
    "    _, mask = explanation.get_image_and_mask(\n",
    "        label=explanation.top_labels[0],\n",
    "        positive_only=True,\n",
    "        hide_rest=False,\n",
    "        num_features=5\n",
    "    )\n",
    "    masks.append(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(single_image), len(mask))  # Ensure they match in length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "plt.title(f\"Explanation for Image {i+1}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = remove_non_important_features(\n",
    "    images=single_image,\n",
    "    masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/Lime'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_test_feature = np.array(processed_images)\n",
    "lime_test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    imshow(lime_test_feature[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train the new dataset on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/Lime/'\n",
    "new_prefix_ = 'ct_covid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_lime_features_df =  processImages(new_path_,1)\n",
    "ct_covid_lime_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_lime_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_lime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_x = cvd_imgs_dataset_lime.iloc[:,:-1].to_numpy().reshape((2473,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime['class_label'])\n",
    "cvd_imgs_dataset_lime\n",
    "cvd_imgs_dataset_lime = pd.get_dummies(cvd_imgs_dataset_lime, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_y = np.array(cvd_imgs_dataset_lime[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    imshow(input_data_lime_x[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_lime, test_features_lime, train_labels_lime, test_labels_lime = train_test_split(\n",
    "    input_data_lime_x, output_label_lime_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_lime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_lime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 200\n",
    "# data_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         tf.keras.layers.Normalization(),\n",
    "#         tf.keras.layers.Resizing(image_size, image_size),\n",
    "#         tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "#         tf.keras.layers.RandomRotation(factor=0.02),\n",
    "#         tf.keras.layers.RandomZoom(\n",
    "#             height_factor=0.2, width_factor=0.2\n",
    "#         ),\n",
    "#     ],\n",
    "#     name=\"data_augmentation\",\n",
    "# )\n",
    "# # Compute the mean and the variance of the training data for normalization.\n",
    "# data_augmentation.layers[0].adapt(train_features_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "    '''\n",
    "    CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "        model            = CNN Model\n",
    "        num_hl           = Number of hidden layers\n",
    "        hl_list          = List of hidden layer units\n",
    "        hl_conv_activation = Activation function for convolutional layers\n",
    "        ol_activation    = Activation function for the output layer\n",
    "        dropout_val      = List of dropout values for hidden layers\n",
    "        inputShape       = Shape of the input data (e.g., (300, 300, 1))\n",
    "        filters_         = Number of filters for Conv layers\n",
    "        kernel_size_     = Size of the kernel (height, width)\n",
    "        stride_poolSize  = Stride and pool size (same value for both)\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    \n",
    "    # Augment data (assuming data_augmentation is predefined)\n",
    "    #augmented = data_augmentation(inputs)\n",
    "    \n",
    "    # Convolutional Layer 1 with Max-Pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(inputs)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    # Convolutional Layer 2 with Max-Pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_2_max_pool)\n",
    "    \n",
    "    # Flattening the output of the last convolutional block\n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    # Adding Dense and Dropout Layers\n",
    "    dense_layers = flatten_layer\n",
    "    for i in range(num_hl):\n",
    "        dense_layers = tf.keras.layers.Dense(units=hl_list[i], activation=hl_conv_activation)(dense_layers)\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_val[i])(dense_layers)\n",
    "    \n",
    "    # Output Layer\n",
    "    logits = tf.keras.layers.Dense(units=2, activation=ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def compile_model(model, loss_, learningRate, metrics_):\n",
    "    '''\n",
    "    def compile_model(model, loss_, learningRate, metrics_):\n",
    "    model: the model to compile\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: list of metrics to track (e.g., ['accuracy'])\n",
    "    '''\n",
    "    # Use the legacy Adam optimizer if you're on an M1/M2 Mac\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate, weight_decay=0.001)\n",
    "\n",
    "    # Alternatively, if needed, you can enable eager execution\n",
    "    # tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_, metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "  '''\n",
    "  def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "  '''\n",
    "  history = model.fit(\n",
    "    train_features_lime, \n",
    "    train_labels_lime,\n",
    "    validation_split = val_split_size,\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluateModel_loss(history):\n",
    "    print()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training loss','Validation loss'], loc = 'upper left')\n",
    "    plt.savefig(\"Training_validation_loss_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    print()\n",
    "def evaluateModel_accuracy(history):\n",
    "    print()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training Accuracy','Validation Accuracy'], loc = 'upper left')\n",
    "    #plt.show()\n",
    "    plt.savefig(\"Training_validation_accuracy_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()\n",
    "\n",
    "print(CNN_Mod.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "cnn_1  = tf.keras.models.Sequential()\n",
    "cnn_1  = CNN_Mod(cnn_1, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_1  = compile_model(cnn_1,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "history = buildModel(cnn_1,0.20,128,100,10, \"val_loss\", \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the intersection between LIME and SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(masker))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]\n",
    "len(masker_shap_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]\n",
    "len(test_features_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_masks = masks_\n",
    "(lime_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def intersection_shap_lime(images, shap_masks, lime_masks, method='intersection', save=False, save_path='shap_lime_intersection'):\n",
    "    \"\"\"\n",
    "    Finds and saves the intersection between SHAP and LIME masks for a dataset of images.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list or np.ndarray): List or array of original images.\n",
    "    - shap_masks (list or np.ndarray): List or array of SHAP masks (1 for important, 0 for unimportant).\n",
    "    - lime_masks (list or np.ndarray): List or array of LIME masks (1 for important, 0 for unimportant).\n",
    "    - method (str): Method to highlight intersection ('intersection', 'shap_only', 'lime_only').\n",
    "    - save (bool): Whether to save the images showing the intersection.\n",
    "    - save_path (str): Directory to save images if save is True.\n",
    "\n",
    "    Returns:\n",
    "    - intersected_images (list): List of images with intersected regions highlighted.\n",
    "    \"\"\"\n",
    "    if len(images) != len(shap_masks) or len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images, SHAP masks, and LIME masks must be the same.\")\n",
    "\n",
    "    # Create save directory if needed\n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    intersected_images = []\n",
    "\n",
    "    for idx, (image, shap_mask, lime_mask) in enumerate(zip(images, shap_masks, lime_masks)):\n",
    "        # Check if the mask dimensions match the image\n",
    "        if image.shape[:2] != shap_mask.shape or image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and mask dimensions mismatch at index {idx}.\")\n",
    "\n",
    "        # Intersection of SHAP and LIME masks\n",
    "        if method == 'intersection':\n",
    "            intersect_mask = np.logical_and(shap_mask, lime_mask)\n",
    "        elif method == 'shap_only':\n",
    "            intersect_mask = np.logical_and(shap_mask, np.logical_not(lime_mask))\n",
    "        elif method == 'lime_only':\n",
    "            intersect_mask = np.logical_and(lime_mask, np.logical_not(shap_mask))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Available methods: 'intersection', 'shap_only', 'lime_only'.\")\n",
    "\n",
    "        # Create an overlay on the image to highlight intersected regions\n",
    "        intersected_image = image.copy()\n",
    "        intersected_image[intersect_mask == 0] = 0  # Set non-intersected regions to black\n",
    "\n",
    "        intersected_images.append(intersected_image)\n",
    "\n",
    "        # Save intersected image if required\n",
    "        if save:\n",
    "            plt.imsave(f\"{save_path}/intersected_image_{idx}.png\", intersected_image)\n",
    "            print(f\"Saved intersected image at: {save_path}/intersected_image_{idx}.png\")\n",
    "\n",
    "    return intersected_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Usage\n",
    "# Assume images, shap_masks, and lime_masks are already loaded and are lists of images/masks\n",
    "# Each mask should have values of 1 for important and 0 for unimportant regions.\n",
    "\n",
    "images = [np.random.rand(64, 64, 3) for _ in range(5)]  # Example dataset\n",
    "shap_masks = [np.random.randint(0, 2, size=(64, 64)) for _ in range(5)]  # Simulated SHAP masks\n",
    "lime_masks = [np.random.randint(0, 2, size=(64, 64)) for _ in range(5)]  # Simulated LIME masks\n",
    "\n",
    "# Find the intersection\n",
    "intersected_images = intersection_shap_lime(images, shap_masks, lime_masks, method='intersection', save=True, save_path='shap_lime_intersection')\n",
    "\n",
    "# Display results\n",
    "for i, img in enumerate(intersected_images):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Intersected Image {i}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = intersection_shap_lime(test_features_[:300], masker_shap_[:300], lime_masks, method='intersection', save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Train the new dataset on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_intersection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((2473,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    imshow(input_data_intersection_x[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_intersection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_intersection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "    '''\n",
    "    CNN_Mod(model, num_hl, hl_list, hl_conv_activation, ol_activation, dropout_val, inputShape, filters_, kernel_size_, stride_poolSize):\n",
    "        model            = CNN Model\n",
    "        num_hl           = Number of hidden layers\n",
    "        hl_list          = List of hidden layer units\n",
    "        hl_conv_activation = Activation function for convolutional layers\n",
    "        ol_activation    = Activation function for the output layer\n",
    "        dropout_val      = List of dropout values for hidden layers\n",
    "        inputShape       = Shape of the input data (e.g., (300, 300, 1))\n",
    "        filters_         = Number of filters for Conv layers\n",
    "        kernel_size_     = Size of the kernel (height, width)\n",
    "        stride_poolSize  = Stride and pool size (same value for both)\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    \n",
    "    # Augment data (assuming data_augmentation is predefined)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    # Convolutional Layer 1 with Max-Pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(inputs)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    # Convolutional Layer 2 with Max-Pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters=filters_, kernel_size=kernel_size_, \n",
    "                                          strides=stride_poolSize, activation=hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPooling2D(pool_size=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_2_max_pool)\n",
    "    \n",
    "    # Flattening the output of the last convolutional block\n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    # Adding Dense and Dropout Layers\n",
    "    dense_layers = flatten_layer\n",
    "    for i in range(num_hl):\n",
    "        dense_layers = tf.keras.layers.Dense(units=hl_list[i], activation=hl_conv_activation)(dense_layers)\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_val[i])(dense_layers)\n",
    "    \n",
    "    # Output Layer\n",
    "    logits = tf.keras.layers.Dense(units=2, activation=ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def compile_model(model, loss_, learningRate, metrics_):\n",
    "    '''\n",
    "    def compile_model(model, loss_, learningRate, metrics_):\n",
    "    model: the model to compile\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: list of metrics to track (e.g., ['accuracy'])\n",
    "    '''\n",
    "    # Use the legacy Adam optimizer if you're on an M1/M2 Mac\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate, weight_decay=0.001)\n",
    "\n",
    "    # Alternatively, if needed, you can enable eager execution\n",
    "    # tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_, metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "  '''\n",
    "  def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "  '''\n",
    "  history = model.fit(\n",
    "    train_features_intersection, \n",
    "    train_labels_intersection,\n",
    "    validation_split = val_split_size,\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluateModel_loss(history):\n",
    "    print()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training loss','Validation loss'], loc = 'upper left')\n",
    "    plt.savefig(\"Training_validation_loss_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    print()\n",
    "def evaluateModel_accuracy(history):\n",
    "    print()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training Accuracy','Validation Accuracy'], loc = 'upper left')\n",
    "    #plt.show()\n",
    "    plt.savefig(\"Training_validation_accuracy_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()\n",
    "\n",
    "print(CNN_Mod.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "cnn_1  = tf.keras.models.Sequential()\n",
    "cnn_1  = CNN_Mod(cnn_1, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_1  = compile_model(cnn_1,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "history = buildModel(cnn_1,0.20,128,100,10, \"val_loss\", \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 200\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Normalization(),\n",
    "        tf.keras.layers.Resizing(image_size, image_size),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Mod(model, num_hl, hl_list,hl_conv_activation, ol_activation, dropout_val, inputShape,filters_,kernel_size_,stride_poolSize):\n",
    "    '''\n",
    "    CNN_Mod(model, num_hl, hl_list,hl_conv_activation, ol_activation, dropout_val, inputShape,filters_,kernel_size_,stride_poolSize):\n",
    "        model = CNN Model\n",
    "        num_hl = number of hidden layers\n",
    "        hl_list = list of hidden layers\n",
    "        hl_activation = hidden layer activation function\n",
    "        out_activation = output layer activation function\n",
    "        dropout_val = Dropout value\n",
    "        inputShape = shape of input layer\n",
    "        filters_ = filter size of Conv layer\n",
    "        kernel_size_ = size of kernel (x,x)\n",
    "        strides_poolSize = strides and max_pool_szie\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(augmented)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[0], activation = hl_conv_activation)(flatten_layer)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[0])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[1], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[1])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[2], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[2])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[3], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[3])(dense_layers)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(units = 2, activation = ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, loss_, learningRate, metrics_):\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest [mse', 'mae'] #since a regression model\n",
    "    '''\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learningRate,weight_decay=0.001),loss=loss_,metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "def buildModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "  '''\n",
    "  def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "  '''\n",
    "  history = model.fit(\n",
    "    train_features, \n",
    "    train_labels,\n",
    "    validation_split = val_split_size,\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluateModel_loss(history):\n",
    "    print()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training loss','Validation loss'], loc = 'upper left')\n",
    "    plt.savefig(\"Training_validation_loss_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    print()\n",
    "def evaluateModel_accuracy(history):\n",
    "    print()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training Accuracy','Validation Accuracy'], loc = 'upper left')\n",
    "    #plt.show()\n",
    "    plt.savefig(\"Training_validation_accuracy_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()\n",
    "\n",
    "print(CNN_Mod.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "#cnn_1  = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters:\n",
    "Hyperparameters:\n",
    "Convolution Layers filter Count: : 100\n",
    "Convolution Layers Kernel Size: : 5\n",
    "Max-pooling_Stride_ Size: : 2\n",
    "Hiddel Layer 1 Units: 2884\n",
    "dropout1: False\n",
    "Hiddel Layer 2 Units: 1288\n",
    "dropout2: True\n",
    "Hiddel Layer 3 Units: 332\n",
    "dropout3: False\n",
    "Hiddel Layer 4 Units: 116\n",
    "dropout4: False\n",
    "lr: 0.0015272304174499124\n",
    "Score: 0.9326164722442627\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "cnn_1  = tf.keras.models.Sequential()\n",
    "cnn_1  = CNN_Mod(cnn_1, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_1  = compile_model(cnn_1,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "history = buildModel(cnn_1,0.20,128,100,10, \"val_loss\", \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(cnn_1, to_file='cnn_1_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.save('cvd_cnn_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.save_weights('cvd_cnn_weight_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predicted = cnn_1.predict((test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = test_features[index].reshape(300, 300)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(test_labels[index].argmax()))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    #sample_image = image_predicted[index].reshape(300, 300)\n",
    "    #plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Predicted:\" + str(np.argmax(label_predicted[index])))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectLabel(x):\n",
    "        if x == 0:\n",
    "            return 'Non-Covid'\n",
    "        if x == 1:\n",
    "            return 'Covid'\n",
    "        \n",
    "def D1_to_D2(y_pred):\n",
    "    y_true = {'y_true':[i[0] for i in y_pred]}\n",
    "    y_true_df = pd.DataFrame(y_true)\n",
    "    y_true_df = pd.get_dummies(y_true_df, columns =['y_true'])\n",
    "    output_label_y = np.array(y_true_df[y_true_df.columns])\n",
    "    return output_label_y\n",
    "\n",
    "def getModelEvaluation(model, test_features_,test_labels_):\n",
    "    predicted_labels = model.predict(np.stack(test_features_))\n",
    "    df = pd.DataFrame(predicted_labels)\n",
    "    df['Predicted_Labels'] = np.array(df.iloc[:,:2]).argmax(axis =1)\n",
    "    df['Predicted_Labels'] = df['Predicted_Labels'].apply(getCorrectLabel)\n",
    "    if (test_labels_.shape[1]==2):\n",
    "        df['Actual_Labels'] = test_labels_[:,:2].argmax(axis = 1)\n",
    "    else:\n",
    "        df['Actual_Labels'] = D1_to_D2(test_labels_)[:,:2].argmax(axis = 1)\n",
    "    df['Actual_Labels'] = df['Actual_Labels'].apply(getCorrectLabel)\n",
    "    # df['Probality'] = np.max(np.array(df.iloc[:,:7]))\n",
    "    #df.drop([0,1,2,3,4,5,6,7,8,9],axis =1,inplace=True)\n",
    "    #print(df.head(100))\n",
    "    cm = pd.crosstab(df.Predicted_Labels, df.Actual_Labels)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    sns.heatmap(cm,annot = True,cmap='Blues')\n",
    "    ax1.set_title('')\n",
    "    # Saving the figure.\n",
    "    plt.savefig(\"test_Confusion_Matrix_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    pred = model.evaluate(test_features_,test_labels_)\n",
    "    print(\"loss = \" + str(pred[0]))\n",
    "    print(\"test accuracy = \" + str(pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "getModelEvaluation(cnn_1,test_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Model on a different Dataset\n",
    "- https://www.kaggle.com/datasets/luisblanche/covidct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n",
    "\n",
    "path1 = '/home/rkannan/Desktop/Cvd19_Classification/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/rkannan/Desktop/Cvd19_Classification/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "def processImage(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    with os.scandir(imgDirPath) as dirs:\n",
    "        for entry in dirs:\n",
    "            img_names.append(entry.name)\n",
    "    #Creating features for images\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        path = imgDirPath + img\n",
    "        cv_img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "        #create features for machine learning\n",
    "        nFeatures = (cv_img2.shape[0]*cv_img2.shape[1])\n",
    "        features = np.reshape(cv_img2, nFeatures)\n",
    "        all_features.append(features)\n",
    "        #print(features.shape)\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]),dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]),dtype=int)\n",
    "    return imgs_df\n",
    "\n",
    "\n",
    "ct_covid_features_df =  processImage(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImage(path2,0)#0 ---> covnid-19 negative\n",
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "display(cvd_imgs_dataset)\n",
    "\n",
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((746,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset_ = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset_[['output_encode_0','output_encode_1']])\n",
    "#output_label_y = np.array(cvd_imgs_dataset[['output_encode']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))\n",
    "#print('Output_y Data Shape for evaluation: \\n{0}'.format(output_label_y_eval.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new = output_label_y\n",
    "x_test_new = input_data_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Trained Model on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_old  = tf.keras.models.Sequential()\n",
    "cnn_old  = CNN_Mod(cnn_old, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_old  = compile_model(cnn_old,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "cnn_old.load_weights('cvd_cnn_weight_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelEvaluation(cnn_old,x_test_new,y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "def KT_hp_build(hp,shape = (300,300,1), dropout_rate = 0.35):\n",
    "    #initiallizing the model\n",
    "    cnn = tf.keras.models.Sequential()\n",
    "    \n",
    "    filters_ = hp.Int(\"Convolution Layers filter Count: \",min_value = 100, max_value = 130, step = 16 )\n",
    "    kernel_size_ = hp.Int(\"Convolution Layers Kernel Size: \",min_value = 5, max_value = 10 ,step = 3)\n",
    "    strides_poolSize = hp.Choice(\"Max-pooling_Stride_ Size: \",[2])\n",
    "    hl_conv_activation = 'relu'\n",
    "    ol_activation = 'softmax'\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(augmented)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 1 Units\",min_value=2500, max_value=3000,step = 16), activation = hl_conv_activation)(flatten_layer)\n",
    "    if hp.Boolean(\"dropout1\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 2 Units\",min_value=1000, max_value=1500,step = 32), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout2\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 3 Units\",min_value=300, max_value=500,step = 32), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout3\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 4 Units\",min_value=100, max_value=150,step = 4), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout4\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(units = 2, activation = ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=logits)\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    learning_rate = hp.Float(\"lr\", min_value=0.001, max_value=0.002, sampling=\"log\")\n",
    "    \n",
    "    cnn.compile(\n",
    "      #optimizer = 'adam', \n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      loss = 'categorical_crossentropy', \n",
    "      metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "#     # First Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu',input_shape = shape))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     # second Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu'))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     # Third Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu'))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     #Flattening\n",
    "#     cnn.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "#     #Full Connection\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 1 Units\",min_value=2500, max_value=3000,step = 16), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout1\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 2 Units\",min_value=1000, max_value=1500,step = 32), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout2\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 3 Units\",min_value=300, max_value=500,step = 32), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout3\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 4 Units\",min_value=100, max_value=150,step = 4), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout4\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "#     #output\n",
    "#     cnn.add(tf.keras.layers.Dense(units = 2, activation = ol_activation))\n",
    "    \n",
    "    \n",
    "#     # Define the optimizer learning rate as a hyperparameter.\n",
    "#     learning_rate = hp.Float(\"lr\", min_value=0.001, max_value=0.002, sampling=\"log\")\n",
    "    \n",
    "#     cnn.compile(\n",
    "#       #optimizer = 'adam', \n",
    "#       optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#       loss = 'categorical_crossentropy', \n",
    "#       metrics = ['accuracy'])\n",
    "#     return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KT_hp_build(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel= KT_hp_build,\n",
    "    objective=\"val_accuracy\", \n",
    "    overwrite = True,# Do not resume the previous search in the same directory.\n",
    "    max_trials= 5,\n",
    "    directory = \"models/param_tuning\",  # Set a directory to store the intermediate results.\n",
    "    project_name= \"param_tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary() #print a summary of the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visuallizing the hyperparameter tuning process\n",
    "tuner.search(train_features, train_labels, \n",
    "             validation_data=(test_features, test_labels),\n",
    "             validation_split=0.20,\n",
    "             batch_size = 128,\n",
    "             epochs = 100,\n",
    "             # Use the TensorBoard callback.\n",
    "             # The logs will be write to \"models/tb_logs\".\n",
    "             callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=10,verbose=1,mode=\"min\")]\n",
    "                #tf.keras.callbacks.TensorBoard(\"models/tb_logs\")],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
