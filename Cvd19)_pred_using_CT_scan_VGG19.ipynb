{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source\n",
    "\n",
    "https://www.kaggle.com/datasets/luisblanche/covidct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "# import tensorflow.keras.backend as K\n",
    "#tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def limitgpu(maxmem):\n",
    "\tgpus = tf.config.list_physical_devices('GPU')\n",
    "\tif gpus:\n",
    "\t\t# Restrict TensorFlow to only allocate a fraction of GPU memory\n",
    "\t\ttry:\n",
    "\t\t\tfor gpu in gpus:\n",
    "\t\t\t\ttf.config.experimental.set_virtual_device_configuration(gpu,\n",
    "\t\t\t\t\t\t[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=maxmem)])\n",
    "\t\texcept RuntimeError as e:\n",
    "\t\t\t# Virtual devices must be set before GPUs have been initialized\n",
    "\t\t\tprint(e)\n",
    "\n",
    "\n",
    "# 1.5GB\n",
    "limitgpu(1024+512) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.18.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime==0.1.1.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming images data to Specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Processing images for machine learning to create two-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing CT_scan Covid-19  CT_scan nonCovid-19 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_covid_features_df.iloc[0].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_noncovid_features_df.iloc[1].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the two data frame to create a dataset ready for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x__.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y__.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Negative Covid-19\n",
    "for i in range(10):\n",
    "    imshow(input_data_x_[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Positive Covid-19\n",
    "for i in range(15):\n",
    "    imshow(input_data_x__[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize first 5 images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    imshow(input_data_x[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "#### **Splitting the dataset into the Training set and Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_, test_features_, train_labels_, test_labels_ = train_test_split(\n",
    "    input_data_x_, output_label_y_, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features__, test_features__, train_labels__, test_labels__ = train_test_split(\n",
    "    input_data_x__, output_label_y__, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features__.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION OF SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs\n",
    "initjs()\n",
    "custom_objects = None\n",
    "#model_mse = tf.keras.models.load_model('VGG19_model.h5', custom_objects=custom_objects, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19(input_shape=None, classes=5, use_soft=True):\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv1', kernel_initializer=\"he_normal\")(img_input)\n",
    "    x = layers.Conv2D(2, (3, 3), activation='relu', padding='same', name='block1_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(4, (3, 3), activation='relu', padding='same', name='block2_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same', name='block3_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv2', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv3', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv4', kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Classification block\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu', name='fc2')(x)\n",
    "\n",
    "    # Output layer\n",
    "    if use_soft:\n",
    "        x = Dense(classes, activation=\"softmax\", name='predictions')(x)\n",
    "    else:\n",
    "        x = Dense(classes, activation=\"linear\", name=\"Z_4\")(x)\n",
    "\n",
    "    modelss = models.Model(img_input, x, name='vgg19')\n",
    "\n",
    "    return modelss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.fit(train_features, train_labels, epochs=20, validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.save('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Explainer for VGG19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG19 = tf.keras.models.load_model('VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Image(\"blur(32,32)\", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_images_as_lists = []\n",
    "image_shape = (300, 300, 1)\n",
    "\n",
    "for image in test_features:\n",
    "    # Ensure the image has the correct shape\n",
    "    if image.shape != image_shape:\n",
    "        raise ValueError(f\"Image shape {image.shape} does not match expected shape {image_shape}\")\n",
    "\n",
    "    # Initialize the masker for a single image\n",
    "    masker = shap.maskers.Image(\"blur(32,32)\", image_shape)\n",
    "\n",
    "    # Apply the masker to each image\n",
    "    masked_image_tuple = masker(image, np.zeros(image.shape[:-1]))  # Adjust the mask as needed\n",
    "    \n",
    "    # Extract the first element of the tuple (which is the masked image)\n",
    "    masked_image = masked_image_tuple[0]\n",
    "    \n",
    "    # Convert the masked image to a list and store it\n",
    "    masked_images_as_lists.append(masked_image.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAP for Positve and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG19 = shap.GradientExplainer(model_VGG19,train_features)\n",
    "explainVGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_ = explainVGG19.shap_values(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explainer and SHAP for Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG19_negative = shap.GradientExplainer(model_VGG19,train_features_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_negative = explainVGG19_negative.shap_values(test_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_negative[:5], test_features_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explainer and SHAP for Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG19_positive = shap.GradientExplainer(model_VGG19,train_features__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_positive = explainVGG19_positive.shap_values(test_features__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_positive[:5], test_features_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mask out the Important regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_and_save_images(images, shap_values, threshold, output_folder):\n",
    "    \"\"\"\n",
    "    Process images by removing non-important features based on SHAP values,\n",
    "    change the background to white, and apply a red color to non-important features.\n",
    "\n",
    "    Parameters:\n",
    "    - images: numpy array of shape (num_images, height, width, channels)\n",
    "    - shap_values: numpy array of shape (num_images, height, width, channels)\n",
    "    - threshold: importance threshold to decide which features are important\n",
    "    - output_folder: directory to save the processed images\n",
    "    \"\"\"\n",
    "\n",
    "    # Create masks based on SHAP values and threshold\n",
    "    important_features_mask = np.abs(shap_values) > threshold\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Process and save each image\n",
    "    for idx, (img, mask) in enumerate(zip(images, important_features_mask)):\n",
    "        # Ensure image and mask are of the same shape\n",
    "        if img.shape != mask.shape:\n",
    "            raise ValueError(f\"Image shape {img.shape} and mask shape {mask.shape} do not match.\")\n",
    "        \n",
    "        # Create a white background\n",
    "        white_background = np.ones_like(img) * 255\n",
    "\n",
    "        # Apply mask to keep important features and set the rest to white\n",
    "        processed_img = np.where(mask, img, white_background)\n",
    "\n",
    "        # Convert non-important features (where mask is False) to red (for RGB images)\n",
    "        if img.shape[-1] == 3:  # If the image has 3 channels (colored image)\n",
    "            red_mask = np.zeros_like(img)\n",
    "            red_mask[..., 2] = 255  # Red color in RGB\n",
    "\n",
    "            # Ensure to only modify non-important features (where mask is False)\n",
    "            processed_img[~mask] = red_mask[~mask]\n",
    "\n",
    "        # Ensure the image is within the range 0-255 for saving\n",
    "        processed_img = processed_img.astype(np.uint8)\n",
    "\n",
    "        # Save the image\n",
    "        output_file = os.path.join(output_folder, f'processed_image_{idx}.png')\n",
    "        cv2.imwrite(output_file, processed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Images for only negative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_negative = shap_values_negative[:, :, :, :, 0]\n",
    "shap_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values_negative), 95)\n",
    "save_path = \"dataset_important_features/VGG19/SHAP/Negative\" \n",
    "process_and_save_images(test_features_, shap_negative, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Images for only positive Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_positive = shap_values_positive[:, :, :, :, 0]\n",
    "shap_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values_positive), 95)\n",
    "save_path = \"dataset_important_features/VGG19/SHAP/Positive\" \n",
    "process_and_save_images(test_features__, shap_negative, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/SHAP/Positive/'\n",
    "new_prefix = 'ct_covid'\n",
    "\n",
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/SHAP/Negative/'\n",
    "new_prefix_ = 'non_ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_ =  processImages(new_path_,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_ =  processImages(new_path__,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_ = [ct_noncovid_features_df_, ct_covid_features_df_]\n",
    "cvd_imgs_dataset_ = pd.concat(cvd_imgs_)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_ = cvd_imgs_dataset_.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_x_ = cvd_imgs_dataset_shap_.iloc[:,:-1].to_numpy().reshape((870,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_['class_label'])\n",
    "cvd_imgs_dataset_shap_\n",
    "cvd_imgs_dataset_shap_ = pd.get_dummies(cvd_imgs_dataset_shap_, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_y_ = np.array(cvd_imgs_dataset_shap_[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_x_.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    imshow(input_data_shap_x_[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap_, test_features_shap_, train_labels_shap_, test_labels_shap_ = train_test_split(\n",
    "    input_data_shap_x_, output_label_shap_y_, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.fit(train_features_shap_, train_labels_shap_, epochs=20, validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_shap = shap_values_[:, :, :, :, 0]\n",
    "my_model_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_values_), 95)\n",
    "save_path = \"dataset_important_features/VGG19/SHAP\"  # Replace with your directory path\n",
    "process_and_save_images(test_features, my_model_shap, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Retrain the model on important images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/SHAP/'\n",
    "new_prefix = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_shap_features_VGG19_df =  processImages(new_path,1)\n",
    "ct_covid_shap_features_VGG19_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_shap_features_VGG19_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_VGG19 = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap_VGG19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_VGG19_x = cvd_imgs_dataset_shap_VGG19.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_VGG19['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_VGG19['class_label'])\n",
    "cvd_imgs_dataset_shap_VGG19\n",
    "cvd_imgs_dataset_shap_VGG19 = pd.get_dummies(cvd_imgs_dataset_shap_VGG19, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_VGG19_y = np.array(cvd_imgs_dataset_shap_VGG19[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_VGG19_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_VGG19_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap_VGG19, test_features_shap_VGG19, train_labels_shap_VGG19, test_labels_shap_VGG19 = train_test_split(\n",
    "    input_data_shap_VGG19_x, output_label_shap_VGG19_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_shap_VGG19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "model_vgg19.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_shap_VGG19, train_labels_shap_VGG19, epochs=10, validation_data=(test_features_shap_VGG19, test_labels_shap_VGG19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Apply LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIME on negative test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_VGG19.predict(images)\n",
    "\n",
    "single_image = test_features_[0] \n",
    "single_image.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features_[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## LIME for postive test sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_VGG19.predict(images)\n",
    "\n",
    "single_image_ = test_features__[0] \n",
    "single_image_.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features_[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image_,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask_VGG16 = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_VGG19 = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters import gaussian\n",
    "import os\n",
    "\n",
    "def apply_lime_mask(images, lime_masks, method='blur', blur_sigma=5, save=False, save_path='dataset_lime_filtered'):\n",
    "    \"\"\"\n",
    "    Removes unimportant regions from images based on LIME masks.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list or np.ndarray): List or array of original images.\n",
    "    - lime_masks (list or np.ndarray): List or array of LIME masks (1 for important regions, 0 for unimportant).\n",
    "    - method (str): Method to fill unimportant regions ('blur', 'mean', or 'zero').\n",
    "    - blur_sigma (int): Sigma for Gaussian blur if 'blur' method is selected.\n",
    "    - save (bool): Whether to save processed images.\n",
    "    - save_path (str): Directory path to save images if save is True.\n",
    "\n",
    "    Returns:\n",
    "    - processed_images (list): List of images with unimportant regions removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify input types\n",
    "    if not isinstance(images, (list, np.ndarray)):\n",
    "        raise TypeError(\"Images should be a list or numpy array of image arrays.\")\n",
    "        \n",
    "    if not isinstance(lime_masks, (list, np.ndarray)):\n",
    "        raise TypeError(\"LIME masks should be a list or numpy array of mask arrays.\")\n",
    "        \n",
    "    if len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images and LIME masks must be the same.\")\n",
    "    \n",
    "    # Create save directory if needed\n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for idx, (image, lime_mask) in enumerate(zip(images, lime_masks)):\n",
    "        # Ensure image and mask dimensions match\n",
    "        if image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and LIME mask at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        # Copy the image for processing\n",
    "        processed_image = image.copy()\n",
    "        \n",
    "        # Apply the chosen method to unimportant regions (where lime_mask == 0)\n",
    "        if method == 'blur':\n",
    "            blurred_image = gaussian(processed_image, sigma=blur_sigma)\n",
    "            processed_image[lime_mask == 0] = blurred_image[lime_mask == 0]\n",
    "        \n",
    "        elif method == 'mean':\n",
    "            mean_value = image.mean(axis=(0, 1), keepdims=True) if image.ndim == 3 else image.mean()\n",
    "            processed_image[lime_mask == 0] = mean_value\n",
    "        \n",
    "        elif method == 'zero':\n",
    "            processed_image[lime_mask == 0] = 0\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid method chosen. Available methods: 'blur', 'mean', 'zero'.\")\n",
    "        \n",
    "        # Append processed image to the list\n",
    "        processed_images.append(processed_image)\n",
    "        \n",
    "        # Save processed image if required\n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(processed_image)\n",
    "            imsave(os.path.join(save_path, f'lime_filtered_image_{idx}.png'), image_to_save, check_contrast=False)\n",
    "            print(f\"Saved: {os.path.join(save_path, f'lime_filtered_image_{idx}.png')}\")\n",
    "    \n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove unimportant regions negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {mask[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG19/LIME/Negative'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove unimportant regions positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image[0].shape}\")\n",
    "print(f\"Original Mask shape: {mask[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask_VGG16, image.shape) for mask, image in zip(mask_VGG16, single_image)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG19/LIME/Positive'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path___ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/LIME/Positive/'\n",
    "new_prefix__ = 'ct_covid'\n",
    "\n",
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/LIME/Negative/'\n",
    "new_prefix__ = 'non_ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_lime =  processImages(new_path___,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_lime =  processImages(new_path__,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_lime = [ct_noncovid_features_df_lime, ct_covid_features_df_lime]\n",
    "cvd_imgs_dataset_lime = pd.concat(cvd_imgs_lime)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime_ = cvd_imgs_dataset_lime.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_lime_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_x = cvd_imgs_dataset_lime_.iloc[:,:-1].to_numpy().reshape((600,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_['class_label'])\n",
    "cvd_imgs_dataset_lime_\n",
    "cvd_imgs_dataset_lime_ = pd.get_dummies(cvd_imgs_dataset_lime_, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_y = np.array(cvd_imgs_dataset_lime_[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_lime, test_features_lime, train_labels_lime, test_labels_lime = train_test_split(\n",
    "    input_data_lime_x, output_label_lime_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19 = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "model_vgg19.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_lime, train_labels_lime, epochs=100, validation_data=(test_features_lime, test_labels_lime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = np.repeat(mask[np.newaxis, :, :], 930, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "def find_intersection_and_save(images, shap_masks, lime_masks, save=False, save_path='dataset_intersection'):\n",
    "    if len(images) != len(shap_masks) or len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images, SHAP masks, and LIME masks must be the same.\")\n",
    "    \n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    intersection_images = []\n",
    "\n",
    "    for idx, (image, shap_mask, lime_mask) in enumerate(zip(images, shap_masks, lime_masks)):\n",
    "        if image.shape[:2] != shap_mask.shape or image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and masks at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        shap_mask = shap_mask.astype(bool)\n",
    "        lime_mask = lime_mask.astype(bool)\n",
    "        \n",
    "        intersection_mask = np.logical_and(shap_mask, lime_mask)\n",
    "        print(f\"Intersection count at index {idx}: {np.sum(intersection_mask)}\")  # Debugging line\n",
    "        \n",
    "        intersection_image = np.copy(image)\n",
    "\n",
    "        if intersection_image.ndim == 2:\n",
    "            intersection_image = np.stack([intersection_image] * 3, axis=-1)\n",
    "\n",
    "        overlay_image = np.copy(intersection_image)\n",
    "        highlight_color = [255, 0, 0]\n",
    "\n",
    "        overlay_image[intersection_mask] = highlight_color\n",
    "\n",
    "        final_image = np.where(intersection_mask[:, :, np.newaxis], overlay_image, intersection_image)\n",
    "\n",
    "        intersection_images.append(final_image)\n",
    "        \n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(final_image)\n",
    "\n",
    "            if len(image_to_save.shape) not in [2, 3]:\n",
    "                raise ValueError(f\"Unexpected image shape for saving: {image_to_save.shape}\")\n",
    "\n",
    "            imsave(os.path.join(save_path, f'intersection_image_{idx}.png'), image_to_save)\n",
    "    \n",
    "    return intersection_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = find_intersection_and_save(test_features_, masker_shap_, new_mask, save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/Intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG19/Intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG19(input_shape=(300, 300, 1), classes=2)\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg19.fit(train_features_intersection, train_labels_intersection, epochs=10, validation_data=(test_features_intersection, test_labels_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
