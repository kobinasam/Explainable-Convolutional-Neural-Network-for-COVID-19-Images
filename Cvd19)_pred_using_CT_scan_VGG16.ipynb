{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source\n",
    "\n",
    "https://www.kaggle.com/datasets/luisblanche/covidct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "# import tensorflow.keras.backend as K\n",
    "#tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def limitgpu(maxmem):\n",
    "\tgpus = tf.config.list_physical_devices('GPU')\n",
    "\tif gpus:\n",
    "\t\t# Restrict TensorFlow to only allocate a fraction of GPU memory\n",
    "\t\ttry:\n",
    "\t\t\tfor gpu in gpus:\n",
    "\t\t\t\ttf.config.experimental.set_virtual_device_configuration(gpu,\n",
    "\t\t\t\t\t\t[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=maxmem)])\n",
    "\t\texcept RuntimeError as e:\n",
    "\t\t\t# Virtual devices must be set before GPUs have been initialized\n",
    "\t\t\tprint(e)\n",
    "\n",
    "\n",
    "# 1.5GB\n",
    "limitgpu(1024+512) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.18.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime==0.1.1.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming images data to Specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_merged/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Processing images for machine learning to create two-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing CT_scan Covid-19  CT_scan nonCovid-19 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_covid_features_df.iloc[0].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(ct_noncovid_features_df.iloc[1].to_numpy().reshape((100,150)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the two data frame to create a dataset ready for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['output_encode_0']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x__.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y__.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Negative Covid-19\n",
    "for i in range(10):\n",
    "    imshow(input_data_x_[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Positive Covid-19\n",
    "for i in range(15):\n",
    "    imshow(input_data_x__[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize first 5 images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    imshow(input_data_x[i])\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "#### **Splitting the dataset into the Training set and Test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_, test_features_, train_labels_, test_labels_ = train_test_split(\n",
    "    input_data_x_, output_label_y_, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features__, test_features__, train_labels__, test_labels__ = train_test_split(\n",
    "    input_data_x__, output_label_y__, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features__.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION OF SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def VGG16():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1), input_shape=(300, 300, 1)))  # Input shape corrected\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(2, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(2, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(4, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=\"he_normal\"))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='inter'))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu', name='fc2'))\n",
    "    model.add(Dense(2, activation='softmax'))  # Updated to 2 classes for binary classification\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features, train_labels, epochs=50, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss.save('VGG16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap import initjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16 = tf.keras.models.load_model('VGG16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainVGG16 = shap.GradientExplainer(model_VGG16,train_features)\n",
    "explainVGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_valuesVGG16 = explainVGG16.shap_values(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_valuesVGG16[:5], test_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP on Negative test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_negative = shap.GradientExplainer(model_VGG16,test_features_)\n",
    "explainVGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_negative = explain_negative.shap_values(test_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP on Positive test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_positive = shap.GradientExplainer(model_VGG16,test_features__)\n",
    "explainVGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_positive = explain_positive.shap_values(test_features__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def process_and_save_images(images, shap_values, threshold, output_folder):\n",
    "    \"\"\"\n",
    "    Process images by removing non-important features based on SHAP values and save the images.\n",
    "\n",
    "    Parameters:\n",
    "    - images: numpy array of shape (num_images, height, width) or (num_images, height, width, channels)\n",
    "    - shap_values: numpy array of shape (num_images, height, width) or (num_images, height, width, channels)\n",
    "    - threshold: importance threshold to decide which features are important\n",
    "    - output_folder: directory to save the processed images\n",
    "    \"\"\"\n",
    "\n",
    "    # Create masks based on SHAP values and threshold\n",
    "    important_features_mask = np.abs(shap_values) > threshold\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Process and save each image\n",
    "    for idx, (img, mask) in enumerate(zip(images, important_features_mask)):\n",
    "        # Ensure image and mask are of the same shape\n",
    "        if img.shape != mask.shape:\n",
    "            raise ValueError(f\"Image shape {img.shape} and mask shape {mask.shape} do not match.\")\n",
    "\n",
    "        # Apply mask to the image\n",
    "        processed_img = img * mask\n",
    "\n",
    "        # Ensure the image is within the range 0-255 for saving\n",
    "        processed_img = processed_img - np.min(processed_img)\n",
    "        processed_img = (processed_img / np.max(processed_img) * 255).astype(np.uint8)\n",
    "\n",
    "        # Save the image\n",
    "        output_file = os.path.join(output_folder, f'processed_image_{idx}.png')\n",
    "        cv2.imwrite(output_file, processed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Images for only negative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_negative = shap_values_negative[:, :, :, :, 0]\n",
    "shap_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(np.abs(shap_negative), 95)\n",
    "save_path = \"dataset_important_features/VGG16/Shap/Negative\" \n",
    "process_and_save_images(test_features_, shap_negative, threshold, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Images for only positive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_positive = shap_values_positive[:, :, :, :, 0]\n",
    "shap_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_ = np.percentile(np.abs(shap_positive), 95)\n",
    "save_path = \"dataset_important_features/VGG16/Shap/Positive\" \n",
    "process_and_save_images(test_features_, shap_positive, threshold_, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path_ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Shap/Positive/'\n",
    "new_prefix = 'ct_covid'\n",
    "\n",
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Shap/Negative/'\n",
    "new_prefix_ = 'non_ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_ =  processImages(new_path_,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_ =  processImages(new_path__,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_ = [ct_noncovid_features_df_, ct_covid_features_df_]\n",
    "cvd_imgs_dataset_ = pd.concat(cvd_imgs_)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap_ = cvd_imgs_dataset_.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shap_x_ = cvd_imgs_dataset_shap_.iloc[:,:-1].to_numpy().reshape((870,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap_['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap_['class_label'])\n",
    "cvd_imgs_dataset_shap_\n",
    "cvd_imgs_dataset_shap_ = pd.get_dummies(cvd_imgs_dataset_shap_, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_shap_y_ = np.array(cvd_imgs_dataset_shap_[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_shap_x_.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_shap_y_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap_, test_features_shap_, train_labels_shap_, test_labels_shap_ = train_test_split(\n",
    "    input_data_shap_x_, output_label_shap_y_, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features_shap_, train_labels_shap_, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying LIME on Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_VGG16.predict(images)\n",
    "\n",
    "single_image_neg = test_features_[0] \n",
    "single_image_neg.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks_neg = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image = test_features[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image_neg,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks_neg.append(mask)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters import gaussian\n",
    "import os\n",
    "\n",
    "def apply_lime_mask(images, lime_masks, method='blur', blur_sigma=5, save=False, save_path='dataset_lime_filtered'):\n",
    "    \"\"\"\n",
    "    Removes unimportant regions from images based on LIME masks.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list or np.ndarray): List or array of original images.\n",
    "    - lime_masks (list or np.ndarray): List or array of LIME masks (1 for important regions, 0 for unimportant).\n",
    "    - method (str): Method to fill unimportant regions ('blur', 'mean', or 'zero').\n",
    "    - blur_sigma (int): Sigma for Gaussian blur if 'blur' method is selected.\n",
    "    - save (bool): Whether to save processed images.\n",
    "    - save_path (str): Directory path to save images if save is True.\n",
    "\n",
    "    Returns:\n",
    "    - processed_images (list): List of images with unimportant regions removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Verify input types\n",
    "    if not isinstance(images, (list, np.ndarray)):\n",
    "        raise TypeError(\"Images should be a list or numpy array of image arrays.\")\n",
    "        \n",
    "    if not isinstance(lime_masks, (list, np.ndarray)):\n",
    "        raise TypeError(\"LIME masks should be a list or numpy array of mask arrays.\")\n",
    "        \n",
    "    if len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images and LIME masks must be the same.\")\n",
    "    \n",
    "    # Create save directory if needed\n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    processed_images = []\n",
    "    \n",
    "    for idx, (image, lime_mask) in enumerate(zip(images, lime_masks)):\n",
    "        # Ensure image and mask dimensions match\n",
    "        if image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and LIME mask at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        # Copy the image for processing\n",
    "        processed_image = image.copy()\n",
    "        \n",
    "        # Apply the chosen method to unimportant regions (where lime_mask == 0)\n",
    "        if method == 'blur':\n",
    "            blurred_image = gaussian(processed_image, sigma=blur_sigma)\n",
    "            processed_image[lime_mask == 0] = blurred_image[lime_mask == 0]\n",
    "        \n",
    "        elif method == 'mean':\n",
    "            mean_value = image.mean(axis=(0, 1), keepdims=True) if image.ndim == 3 else image.mean()\n",
    "            processed_image[lime_mask == 0] = mean_value\n",
    "        \n",
    "        elif method == 'zero':\n",
    "            processed_image[lime_mask == 0] = 0\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid method chosen. Available methods: 'blur', 'mean', 'zero'.\")\n",
    "        \n",
    "        # Append processed image to the list\n",
    "        processed_images.append(processed_image)\n",
    "        \n",
    "        # Save processed image if required\n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(processed_image)\n",
    "            imsave(os.path.join(save_path, f'lime_filtered_image_{idx}.png'), image_to_save, check_contrast=False)\n",
    "            print(f\"Saved: {os.path.join(save_path, f'lime_filtered_image_{idx}.png')}\")\n",
    "    \n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks__ = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_mask(mask, image_shape):\n",
    "    return resize(mask, image_shape[:2], mode='reflect', anti_aliasing=True, preserve_range=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image_neg[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks_neg[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks_neg, single_image_neg)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image_neg,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG16/Lime/Negative'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying LIME on Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries, felzenszwalb\n",
    "\n",
    "# Function to predict on the model\n",
    "def predict_fn(images):\n",
    "    return model_VGG16.predict(images)\n",
    "\n",
    "single_image_pos = test_features__[0] \n",
    "single_image_pos.shape[0]\n",
    "num_images_to_explain = 300\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanations = []\n",
    "masks_pos = []\n",
    "segmentation_fn = lambda x: felzenszwalb(x, scale=100, sigma=0.5, min_size=50)\n",
    "\n",
    "for i in range(num_images_to_explain):\n",
    "    single_image_pos = test_features__[i]  # Select the i-th image in the batch\n",
    "    \n",
    "    # Explain the prediction for the grayscale image\n",
    "    explanation = explainer.explain_instance(single_image_pos,\n",
    "                                             predict_fn, \n",
    "                                             top_labels=10, \n",
    "                                             hide_color=0, \n",
    "                                             num_samples=1000,\n",
    "                                             segmentation_fn=segmentation_fn)\n",
    "    \n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    # Get explanation for the top predicted class\n",
    "    temp, mask_ = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                                positive_only=True, \n",
    "                                                num_features=10, \n",
    "                                                hide_rest=False)\n",
    "    masks_pos.append(mask_)\n",
    "\n",
    "     # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(test_features[i])\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Show the explanation\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(mark_boundaries(single_image.squeeze(), mask), cmap='gray')\n",
    "    plt.title(f\"Explanation for Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions before processing\n",
    "print(f\"Original Image shape: {single_image_pos[0].shape}\")\n",
    "print(f\"Original Mask shape: {masks_pos[0].shape}\")\n",
    "\n",
    "# Resize the mask if needed\n",
    "resized_masks = [resize_mask(mask, image.shape) for mask, image in zip(masks_pos, single_image_pos)]\n",
    "\n",
    "# Process images with the resized masks\n",
    "processed_images = apply_lime_mask(\n",
    "    images=single_image_pos,\n",
    "    lime_masks=resized_masks,\n",
    "    method='mean',       # Options: 'mean', 'zero', 'blur'\n",
    "    blur_sigma=3,        # Used only if method='blur'\n",
    "    save=True,           # Set to True to save images\n",
    "    save_path='dataset_important_features/VGG16/Lime/Positive'  # Directory to save images\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path___ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Lime/Positive/'\n",
    "new_prefix__ = 'ct_covid'\n",
    "\n",
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Lime/Negative/'\n",
    "new_prefix__ = 'non_ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_lime =  processImages(new_path___,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_lime =  processImages(new_path__,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_lime = [ct_noncovid_features_df_lime, ct_covid_features_df_lime]\n",
    "cvd_imgs_dataset_lime = pd.concat(cvd_imgs_lime)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_lime_ = cvd_imgs_dataset_lime.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_lime_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_lime_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_lime_x = cvd_imgs_dataset_lime_.iloc[:,:-1].to_numpy().reshape((600,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_lime_['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_lime_['class_label'])\n",
    "cvd_imgs_dataset_lime_\n",
    "cvd_imgs_dataset_lime_ = pd.get_dummies(cvd_imgs_dataset_lime_, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_lime_y = np.array(cvd_imgs_dataset_lime_[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_lime_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_lime_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_lime, test_features_lime, train_labels_lime, test_labels_lime = train_test_split(\n",
    "    input_data_lime_x, output_label_lime_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelss = VGG16()\n",
    "modelss.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "modelss.fit(train_features_lime, train_labels_lime, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding the Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Image(\"blur(32,32)\", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_images_as_lists = []\n",
    "image_shape = (300, 300, 1)\n",
    "\n",
    "for image in test_features:\n",
    "    # Ensure the image has the correct shape\n",
    "    if image.shape != image_shape:\n",
    "        raise ValueError(f\"Image shape {image.shape} does not match expected shape {image_shape}\")\n",
    "\n",
    "    # Initialize the masker for a single image\n",
    "    masker = shap.maskers.Image(\"blur(32,32)\", image_shape)\n",
    "\n",
    "    # Apply the masker to each image\n",
    "    masked_image_tuple = masker(image, np.zeros(image.shape[:-1]))  # Adjust the mask as needed\n",
    "    \n",
    "    # Extract the first element of the tuple (which is the masked image)\n",
    "    masked_image = masked_image_tuple[0]\n",
    "    \n",
    "    # Convert the masked image to a list and store it\n",
    "    masked_images_as_lists.append(masked_image.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "def find_intersection_and_save(images, shap_masks, lime_masks, save=False, save_path='dataset_intersection'):\n",
    "    if len(images) != len(shap_masks) or len(images) != len(lime_masks):\n",
    "        raise ValueError(\"The number of images, SHAP masks, and LIME masks must be the same.\")\n",
    "    \n",
    "    if save and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    intersection_images = []\n",
    "\n",
    "    for idx, (image, shap_mask, lime_mask) in enumerate(zip(images, shap_masks, lime_masks)):\n",
    "        if image.shape[:2] != shap_mask.shape or image.shape[:2] != lime_mask.shape:\n",
    "            raise ValueError(f\"Image and masks at index {idx} have mismatched dimensions.\")\n",
    "        \n",
    "        shap_mask = shap_mask.astype(bool)\n",
    "        lime_mask = lime_mask.astype(bool)\n",
    "        \n",
    "        intersection_mask = np.logical_and(shap_mask, lime_mask)\n",
    "        print(f\"Intersection count at index {idx}: {np.sum(intersection_mask)}\")  # Debugging line\n",
    "        \n",
    "        intersection_image = np.copy(image)\n",
    "\n",
    "        if intersection_image.ndim == 2:\n",
    "            intersection_image = np.stack([intersection_image] * 3, axis=-1)\n",
    "\n",
    "        overlay_image = np.copy(intersection_image)\n",
    "        highlight_color = [255, 0, 0]\n",
    "\n",
    "        overlay_image[intersection_mask] = highlight_color\n",
    "\n",
    "        final_image = np.where(intersection_mask[:, :, np.newaxis], overlay_image, intersection_image)\n",
    "\n",
    "        intersection_images.append(final_image)\n",
    "        \n",
    "        if save:\n",
    "            image_to_save = img_as_ubyte(final_image)\n",
    "\n",
    "            if len(image_to_save.shape) not in [2, 3]:\n",
    "                raise ValueError(f\"Unexpected image shape for saving: {image_to_save.shape}\")\n",
    "\n",
    "            imsave(os.path.join(save_path, f'intersection_image_{idx}.png'), image_to_save)\n",
    "    \n",
    "    return intersection_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap = np.array(masked_images_as_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_shap_ = masker_shap[:, 0, :, :]\n",
    "len(masker_shap_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_ = test_features[:, :, :, 0]\n",
    "len(test_features_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask = np.repeat(mask[np.newaxis, :, :], 930, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_images = find_intersection_and_save(test_features_, masker_shap_, new_mask, save=True, save_path=\"/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Intersection/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train on the intersection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path__ = '/home/maxwellsam/SHAP_LIME_COVID-19/backup_93_perc/dataset_important_features/VGG16/Intersection/'\n",
    "new_prefix_ = 'ct_covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_intersection_features_df =  processImages(new_path__,1)\n",
    "ct_covid_intersection_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_intersection = [ct_noncovid_features_df, ct_covid_intersection_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs_intersection)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_intersection = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_intersection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_intersection_x = cvd_imgs_dataset_intersection.iloc[:,:-1].to_numpy().reshape((3103,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_intersection['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_intersection['class_label'])\n",
    "cvd_imgs_dataset_intersection\n",
    "cvd_imgs_dataset_intersection = pd.get_dummies(cvd_imgs_dataset_intersection, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_intersection_y = np.array(cvd_imgs_dataset_intersection[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_intersection_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_intersection_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_intersection, test_features_intersection, train_labels_intersection, test_labels_intersection = train_test_split(\n",
    "    input_data_intersection_x, output_label_intersection_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inter = VGG16()\n",
    "model_inter.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_inter.fit(train_features_intersection, train_labels_intersection, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 200\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Normalization(),\n",
    "        tf.keras.layers.Resizing(image_size, image_size),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Mod(model, num_hl, hl_list,hl_conv_activation, ol_activation, dropout_val, inputShape,filters_,kernel_size_,stride_poolSize):\n",
    "    '''\n",
    "    CNN_Mod(model, num_hl, hl_list,hl_conv_activation, ol_activation, dropout_val, inputShape,filters_,kernel_size_,stride_poolSize):\n",
    "        model = CNN Model\n",
    "        num_hl = number of hidden layers\n",
    "        hl_list = list of hidden layers\n",
    "        hl_activation = hidden layer activation function\n",
    "        out_activation = output layer activation function\n",
    "        dropout_val = Dropout value\n",
    "        inputShape = shape of input layer\n",
    "        filters_ = filter size of Conv layer\n",
    "        kernel_size_ = size of kernel (x,x)\n",
    "        strides_poolSize = strides and max_pool_szie\n",
    "    '''\n",
    "    assert(num_hl == len(hl_list))\n",
    "    assert(num_hl == len(dropout_val))\n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(augmented)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[0], activation = hl_conv_activation)(flatten_layer)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[0])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[1], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[1])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[2], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[2])(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hl_list[3], activation = hl_conv_activation)(dense_layers)\n",
    "    dense_layers = tf.keras.layers.Dropout(dropout_val[3])(dense_layers)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(units = 2, activation = ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=logits)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, loss_, learningRate, metrics_):\n",
    "    '''\n",
    "    def compile_model(DNN, loss_, learningRate, metrics_):\n",
    "    DNN: the model\n",
    "    loss_: the loss function\n",
    "    learningRate: learning rate\n",
    "    metrics_: metrics of interest [mse', 'mae'] #since a regression model\n",
    "    '''\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=learningRate,weight_decay=0.001),loss=loss_,metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "def buildModel(model, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "  '''\n",
    "  def buildModel(DNN, val_split_size, batch_size_,numEpochs, patience_, monitor_, mode):\n",
    "    DNN: DNN which the model\n",
    "    val_split_size: the validation split)\n",
    "    batch_size_: batch_size\n",
    "    numEpochs: number of epochs\n",
    "    patience_: patience of call back\n",
    "    monitor_: monitor (objective of callback)\n",
    "    mode: mode (min, max, auto)\n",
    "  '''\n",
    "  history = model.fit(\n",
    "    train_features, \n",
    "    train_labels,\n",
    "    validation_split = val_split_size,\n",
    "    batch_size = batch_size_, \n",
    "    epochs = numEpochs,\n",
    "    callbacks = [\n",
    "      tf.keras.callbacks.EarlyStopping(monitor= monitor_,patience=patience_,verbose=1,mode=mode),#monitoring loss mode should be min [---val_acc--]\n",
    "      #tf.keras.callbacks.ModelCheckpoint(filepath='./TrainedModels/model.{epoch:02d}-{val_loss:.2f}.h5',save_best_only=True),\n",
    "      #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ]\n",
    "    )\n",
    "  \n",
    "  return history\n",
    "\n",
    "def evaluateModel_loss(history):\n",
    "    print()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training loss','Validation loss'], loc = 'upper left')\n",
    "    plt.savefig(\"Training_validation_loss_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "    print()\n",
    "def evaluateModel_accuracy(history):\n",
    "    print()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['Training Accuracy','Validation Accuracy'], loc = 'upper left')\n",
    "    #plt.show()\n",
    "    plt.savefig(\"Training_validation_accuracy_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()\n",
    "\n",
    "print(CNN_Mod.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "#cnn_1  = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters:\n",
    "Hyperparameters:\n",
    "Convolution Layers filter Count: : 100\n",
    "Convolution Layers Kernel Size: : 5\n",
    "Max-pooling_Stride_ Size: : 2\n",
    "Hiddel Layer 1 Units: 2884\n",
    "dropout1: False\n",
    "Hiddel Layer 2 Units: 1288\n",
    "dropout2: True\n",
    "Hiddel Layer 3 Units: 332\n",
    "dropout3: False\n",
    "Hiddel Layer 4 Units: 116\n",
    "dropout4: False\n",
    "lr: 0.0015272304174499124\n",
    "Score: 0.9326164722442627\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing and building the model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "cnn_1  = tf.keras.models.Sequential()\n",
    "cnn_1  = CNN_Mod(cnn_1, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_1  = compile_model(cnn_1,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "history = buildModel(cnn_1,0.20,128,100,10, \"val_loss\", \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateModel_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(cnn_1, to_file='cnn_1_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.save('cvd_cnn_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1.save_weights('cvd_cnn_weight_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predicted = cnn_1.predict((test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = test_features[index].reshape(300, 300)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(test_labels[index].argmax()))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    #sample_image = image_predicted[index].reshape(300, 300)\n",
    "    #plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.title(\"Predicted:\" + str(np.argmax(label_predicted[index])))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectLabel(x):\n",
    "        if x == 0:\n",
    "            return 'Non-Covid'\n",
    "        if x == 1:\n",
    "            return 'Covid'\n",
    "        \n",
    "def D1_to_D2(y_pred):\n",
    "    y_true = {'y_true':[i[0] for i in y_pred]}\n",
    "    y_true_df = pd.DataFrame(y_true)\n",
    "    y_true_df = pd.get_dummies(y_true_df, columns =['y_true'])\n",
    "    output_label_y = np.array(y_true_df[y_true_df.columns])\n",
    "    return output_label_y\n",
    "\n",
    "def getModelEvaluation(model, test_features_,test_labels_):\n",
    "    predicted_labels = model.predict(np.stack(test_features_))\n",
    "    df = pd.DataFrame(predicted_labels)\n",
    "    df['Predicted_Labels'] = np.array(df.iloc[:,:2]).argmax(axis =1)\n",
    "    df['Predicted_Labels'] = df['Predicted_Labels'].apply(getCorrectLabel)\n",
    "    if (test_labels_.shape[1]==2):\n",
    "        df['Actual_Labels'] = test_labels_[:,:2].argmax(axis = 1)\n",
    "    else:\n",
    "        df['Actual_Labels'] = D1_to_D2(test_labels_)[:,:2].argmax(axis = 1)\n",
    "    df['Actual_Labels'] = df['Actual_Labels'].apply(getCorrectLabel)\n",
    "    # df['Probality'] = np.max(np.array(df.iloc[:,:7]))\n",
    "    #df.drop([0,1,2,3,4,5,6,7,8,9],axis =1,inplace=True)\n",
    "    #print(df.head(100))\n",
    "    cm = pd.crosstab(df.Predicted_Labels, df.Actual_Labels)\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    sns.heatmap(cm,annot = True,cmap='Blues')\n",
    "    ax1.set_title('')\n",
    "    # Saving the figure.\n",
    "    plt.savefig(\"test_Confusion_Matrix_with_i.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    pred = model.evaluate(test_features_,test_labels_)\n",
    "    print(\"loss = \" + str(pred[0]))\n",
    "    print(\"test accuracy = \" + str(pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "getModelEvaluation(cnn_1,test_features,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Trained Model on a different Dataset\n",
    "- https://www.kaggle.com/datasets/luisblanche/covidct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n",
    "\n",
    "path1 = '/home/rkannan/Desktop/Cvd19_Classification/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/rkannan/Desktop/Cvd19_Classification/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "def processImage(imgDirPath,binary_label):\n",
    "    img_names = list()\n",
    "    with os.scandir(imgDirPath) as dirs:\n",
    "        for entry in dirs:\n",
    "            img_names.append(entry.name)\n",
    "    #Creating features for images\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        path = imgDirPath + img\n",
    "        cv_img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "        #create features for machine learning\n",
    "        nFeatures = (cv_img2.shape[0]*cv_img2.shape[1])\n",
    "        features = np.reshape(cv_img2, nFeatures)\n",
    "        all_features.append(features)\n",
    "        #print(features.shape)\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]),dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]),dtype=int)\n",
    "    return imgs_df\n",
    "\n",
    "\n",
    "ct_covid_features_df =  processImage(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImage(path2,0)#0 ---> covnid-19 negative\n",
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "display(cvd_imgs_dataset)\n",
    "\n",
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((746,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset_ = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset_[['output_encode_0','output_encode_1']])\n",
    "#output_label_y = np.array(cvd_imgs_dataset[['output_encode']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))\n",
    "#print('Output_y Data Shape for evaluation: \\n{0}'.format(output_label_y_eval.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new = output_label_y\n",
    "x_test_new = input_data_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Trained Model on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_old  = tf.keras.models.Sequential()\n",
    "cnn_old  = CNN_Mod(cnn_old, 4, [288, 128, 33, 11],'relu','softmax',[0.00,0.35,0.00,0.00],(300,300,1),100,5,2)\n",
    "#compile and run model here\n",
    "cnn_old  = compile_model(cnn_old,'categorical_crossentropy',0.0015272304174499124,['accuracy'])\n",
    "cnn_old.load_weights('cvd_cnn_weight_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelEvaluation(cnn_old,x_test_new,y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "def KT_hp_build(hp,shape = (300,300,1), dropout_rate = 0.35):\n",
    "    #initiallizing the model\n",
    "    cnn = tf.keras.models.Sequential()\n",
    "    \n",
    "    filters_ = hp.Int(\"Convolution Layers filter Count: \",min_value = 100, max_value = 130, step = 16 )\n",
    "    kernel_size_ = hp.Int(\"Convolution Layers Kernel Size: \",min_value = 5, max_value = 10 ,step = 3)\n",
    "    strides_poolSize = hp.Choice(\"Max-pooling_Stride_ Size: \",[2])\n",
    "    hl_conv_activation = 'relu'\n",
    "    ol_activation = 'softmax'\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_1 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(augmented)\n",
    "    conv_layer_1_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_1)\n",
    "    x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    #connnnvolultion layer 1 with Map-pooling\n",
    "    conv_layer_2 = tf.keras.layers.Conv2D(filters = filters_,kernel_size = kernel_size_,strides = stride_poolSize, activation = hl_conv_activation)(x1)\n",
    "    conv_layer_2_max_pool = tf.keras.layers.MaxPool2D(strides=stride_poolSize)(conv_layer_2)\n",
    "    x2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(conv_layer_1_max_pool)\n",
    "    \n",
    "    flatten_layer = tf.keras.layers.Flatten()(x2)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 1 Units\",min_value=2500, max_value=3000,step = 16), activation = hl_conv_activation)(flatten_layer)\n",
    "    if hp.Boolean(\"dropout1\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 2 Units\",min_value=1000, max_value=1500,step = 32), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout2\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 3 Units\",min_value=300, max_value=500,step = 32), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout3\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    dense_layers = tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 4 Units\",min_value=100, max_value=150,step = 4), activation = hl_conv_activation)(dense_layers)\n",
    "    if hp.Boolean(\"dropout4\"):\n",
    "        dense_layers = tf.keras.layers.Dropout(dropout_rate)(dense_layers)\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(units = 2, activation = ol_activation)(dense_layers)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=logits)\n",
    "    # Define the optimizer learning rate as a hyperparameter.\n",
    "    learning_rate = hp.Float(\"lr\", min_value=0.001, max_value=0.002, sampling=\"log\")\n",
    "    \n",
    "    cnn.compile(\n",
    "      #optimizer = 'adam', \n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      loss = 'categorical_crossentropy', \n",
    "      metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "#     # First Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu',input_shape = shape))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     # second Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu'))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     # Third Convolution Layer with Max-Pooling\n",
    "#     cnn.add(tf.keras.layers.Conv2D(filters = filters_val,kernel_size = kernel_size_val,strides = strides_poolSize, activation ='relu'))\n",
    "#     cnn.add(tf.keras.layers.MaxPool2D(pool_size=strides_poolSize, strides=strides_poolSize))\n",
    "    \n",
    "#     #Flattening\n",
    "#     cnn.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "#     #Full Connection\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 1 Units\",min_value=2500, max_value=3000,step = 16), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout1\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 2 Units\",min_value=1000, max_value=1500,step = 32), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout2\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 3 Units\",min_value=300, max_value=500,step = 32), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout3\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "#     cnn.add(tf.keras.layers.Dense(units = hp.Int(\"Hiddel Layer 4 Units\",min_value=100, max_value=150,step = 4), activation = hl_conv_activation))\n",
    "#     if hp.Boolean(\"dropout4\"):\n",
    "#         cnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "#     #output\n",
    "#     cnn.add(tf.keras.layers.Dense(units = 2, activation = ol_activation))\n",
    "    \n",
    "    \n",
    "#     # Define the optimizer learning rate as a hyperparameter.\n",
    "#     learning_rate = hp.Float(\"lr\", min_value=0.001, max_value=0.002, sampling=\"log\")\n",
    "    \n",
    "#     cnn.compile(\n",
    "#       #optimizer = 'adam', \n",
    "#       optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#       loss = 'categorical_crossentropy', \n",
    "#       metrics = ['accuracy'])\n",
    "#     return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KT_hp_build(kt.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel= KT_hp_build,\n",
    "    objective=\"val_accuracy\", \n",
    "    overwrite = True,# Do not resume the previous search in the same directory.\n",
    "    max_trials= 5,\n",
    "    directory = \"models/param_tuning\",  # Set a directory to store the intermediate results.\n",
    "    project_name= \"param_tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary() #print a summary of the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visuallizing the hyperparameter tuning process\n",
    "tuner.search(train_features, train_labels, \n",
    "             validation_data=(test_features, test_labels),\n",
    "             validation_split=0.20,\n",
    "             batch_size = 128,\n",
    "             epochs = 100,\n",
    "             # Use the TensorBoard callback.\n",
    "             # The logs will be write to \"models/tb_logs\".\n",
    "             callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=10,verbose=1,mode=\"min\")]\n",
    "                #tf.keras.callbacks.TensorBoard(\"models/tb_logs\")],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
